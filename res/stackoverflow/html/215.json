{
    "items": [
        {
            "tags": [
                "python",
                "reduce"
            ],
            "owner": {
                "reputation": 7,
                "user_id": 2681639,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/3801ad8fdea1ea3e8d2dc4a15084503e?s=128&d=identicon&r=PG&f=1",
                "display_name": "DroidPanda",
                "link": "https://stackoverflow.com/users/2681639/droidpanda"
            },
            "is_answered": false,
            "view_count": 24,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1523524190,
            "creation_date": 1523524190,
            "question_id": 49792486,
            "body_markdown": "While reducing a list in python, I have thought about creating a multiple list reduction and wrote the following snippet.\r\n\r\n    def multiply(a, b): return a * b\r\n    \r\n    def recursive_reduce(reduce_func, *args):\r\n         ret_val = reduce(reduce_func, *args)\r\n         if type(ret_val) == list:\r\n             ret_val = recursive_reduce(reduce_func, ret_val)\r\n         return ret_val\r\n    \r\n    a = [1, 1, 3]\r\n    b = [4, 5, 6]\r\n    \r\n    recursive_reduce(multiply, a, b)\r\n\r\nThis works. However I would like to know whether defining the logic for iteration based on the type of the return value is pythonic or not.\r\n\r\nDo we have any other way which accomplishes recursive reduction in more elegant way?\r\n\r\n",
            "link": "https://stackoverflow.com/questions/49792486/pythonic-way-for-recursive-reduction-in-python",
            "title": "Pythonic way for recursive reduction in python",
            "body": "<p>While reducing a list in python, I have thought about creating a multiple list reduction and wrote the following snippet.</p>\n\n<pre><code>def multiply(a, b): return a * b\n\ndef recursive_reduce(reduce_func, *args):\n     ret_val = reduce(reduce_func, *args)\n     if type(ret_val) == list:\n         ret_val = recursive_reduce(reduce_func, ret_val)\n     return ret_val\n\na = [1, 1, 3]\nb = [4, 5, 6]\n\nrecursive_reduce(multiply, a, b)\n</code></pre>\n\n<p>This works. However I would like to know whether defining the logic for iteration based on the type of the return value is pythonic or not.</p>\n\n<p>Do we have any other way which accomplishes recursive reduction in more elegant way?</p>\n"
        },
        {
            "tags": [
                "python",
                "python-2.7",
                "unicode",
                "encoding"
            ],
            "owner": {
                "reputation": 75,
                "user_id": 6845273,
                "user_type": "registered",
                "profile_image": "https://i.stack.imgur.com/EUooe.jpg?s=128&g=1",
                "display_name": "VeryLazyBoy",
                "link": "https://stackoverflow.com/users/6845273/verylazyboy"
            },
            "is_answered": false,
            "view_count": 26,
            "answer_count": 2,
            "score": 1,
            "last_activity_date": 1523524186,
            "creation_date": 1523510455,
            "last_edit_date": 1523524186,
            "question_id": 49788569,
            "body_markdown": "I have a source file `test.py` encoded in UTF-16BE:\r\n\r\n    # coding=UTF-16BE\r\n\r\n    print &quot;test utf-16&quot;\r\n\r\nWhen I run the following command in my bash:\r\n\r\n    python test.py\r\nNothing printed out in my terminal. Why is that? How should I deal with it? Is it dependent on my bash&#39;s default encoding?\r\n\r\n\r\n\r\n ",
            "link": "https://stackoverflow.com/questions/49788569/how-to-properly-run-python-script-with-utf-16be-encoding",
            "title": "How to properly run python script with UTF-16BE encoding?",
            "body": "<p>I have a source file <code>test.py</code> encoded in UTF-16BE:</p>\n\n<pre><code># coding=UTF-16BE\n\nprint \"test utf-16\"\n</code></pre>\n\n<p>When I run the following command in my bash:</p>\n\n<pre><code>python test.py\n</code></pre>\n\n<p>Nothing printed out in my terminal. Why is that? How should I deal with it? Is it dependent on my bash's default encoding?</p>\n"
        },
        {
            "tags": [
                "concurrency",
                "netcdf",
                "python-xarray"
            ],
            "owner": {
                "reputation": 38,
                "user_id": 356463,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/66292b80a83e9f68880fe5d438660874?s=128&d=identicon&r=PG",
                "display_name": "c_c",
                "link": "https://stackoverflow.com/users/356463/c-c"
            },
            "is_answered": false,
            "view_count": 26,
            "answer_count": 0,
            "score": 1,
            "last_activity_date": 1523524186,
            "creation_date": 1523052535,
            "last_edit_date": 1523524186,
            "question_id": 49701623,
            "body_markdown": "I have a process that grows a NetCDF file `fn` every 5 minutes using `netcdf4.Dataset(fn, mode=a)`. I also have a bokeh server visualization of that NetCDF file using a `xarray.Dataset` (which I want to keep, because it is so convenient).\r\n\r\n**The problem** is that the NetCDF-update-process fails when trying to add new data to `fn` if it is open in my bokeh server process via\r\n\r\n&lt;!-- language: python --&gt;\r\n\r\n    ds = xarray.open_dataset(fn)\r\n\r\nIf I use the option `autoclose`\r\n\r\n&lt;!-- language: python --&gt;\r\n\r\n    ds = xarray.open_dataset(fn, autoclose=True)\r\n\r\nupdating `fn` with the other process while `ds` is &quot;open&quot; in the bokeh server app works, but the updates to the bokeh figure, which pull time slices from `fn`, get very laggy.\r\n\r\n**My question is**: Is there another way to release the lock of the NetCDF file when using `xarray.Dataset`? \r\n\r\nI would not care if the shape of the xarray.Dataset is only updated consistently after reloading the whole bokeh server app.\r\n\r\nThanks!\r\n\r\n**Here is a minimal working example:**\r\n\r\nPut this into a file and let it run:\r\n\r\n&lt;!-- language: python --&gt;\r\n\r\n    import time\r\n    from datetime import datetime\r\n\r\n    import numpy as np\r\n    import netCDF4\r\n\r\n    fn = &#39;my_growing_file.nc&#39;\r\n\r\n    with netCDF4.Dataset(fn, &#39;w&#39;) as nc_fh:\r\n        # create dimensions\r\n        nc_fh.createDimension(&#39;x&#39;, 90)\r\n        nc_fh.createDimension(&#39;y&#39;, 90)\r\n        nc_fh.createDimension(&#39;time&#39;, None)\r\n\r\n        # create variables\r\n        nc_fh.createVariable(&#39;x&#39;, &#39;f8&#39;, (&#39;x&#39;))\r\n        nc_fh.createVariable(&#39;y&#39;, &#39;f8&#39;, (&#39;y&#39;))\r\n        nc_fh.createVariable(&#39;time&#39;, &#39;f8&#39;, (&#39;time&#39;))\r\n        nc_fh.createVariable(&#39;rainfall_amount&#39;,\r\n                             &#39;i2&#39;,\r\n                             (&#39;time&#39;, &#39;y&#39;, &#39;x&#39;),\r\n                             zlib=False,\r\n                             complevel=0,\r\n                             fill_value=-9999,\r\n                             chunksizes=(1, 90, 90))\r\n        nc_fh[&#39;rainfall_amount&#39;].scale_factor = 0.1\r\n        nc_fh[&#39;rainfall_amount&#39;].add_offset = 0\r\n\r\n        nc_fh.set_auto_maskandscale(True)\r\n\r\n        # variable attributes\r\n        nc_fh[&#39;time&#39;].long_name = &#39;Time&#39;\r\n        nc_fh[&#39;time&#39;].standard_name = &#39;time&#39;\r\n        nc_fh[&#39;time&#39;].units = &#39;hours since 2000-01-01 00:50:00.0&#39;\r\n        nc_fh[&#39;time&#39;].calendar = &#39;standard&#39;\r\n\r\n    for i in range(1000):\r\n        with netCDF4.Dataset(fn, &#39;a&#39;) as nc_fh:\r\n            current_length = len(nc_fh[&#39;time&#39;])\r\n\r\n            print(&#39;Appending to NetCDF file {}&#39;.format(fn))\r\n            print(&#39; length of time vector: {}&#39;.format(current_length))\r\n\r\n            if current_length &gt; 0:\r\n                last_time_stamp = netCDF4.num2date(\r\n                    nc_fh[&#39;time&#39;][-1],\r\n                    units=nc_fh[&#39;time&#39;].units,\r\n                    calendar=nc_fh[&#39;time&#39;].calendar)\r\n                print(&#39; last time stamp in NetCDF: {}&#39;.format(str(last_time_stamp)))\r\n            else:\r\n                last_time_stamp = &#39;1900-01-01&#39;\r\n                print(&#39; empty file, starting from scratch&#39;)\r\n\r\n            nc_fh[&#39;time&#39;][i] = netCDF4.date2num(\r\n                datetime.utcnow(),\r\n                units=nc_fh[&#39;time&#39;].units,\r\n                calendar=nc_fh[&#39;time&#39;].calendar)\r\n            nc_fh[&#39;rainfall_amount&#39;][i, :, :] = np.random.rand(90, 90)\r\n\r\n        print(&#39;Sleeping...\\n&#39;)\r\n        time.sleep(3)\r\n\r\n\r\nThen, go to e.g. IPython and open the growing file via:\r\n\r\n&lt;!-- language: python --&gt;\r\n\r\n    ds = xr.open_dataset(&#39;my_growing_file.nc&#39;)\r\n\r\nThis will cause the process that appends to the NetCDF to fail with an output like this:\r\n\r\n&lt;!-- language: python --&gt;\r\n\r\n    Appending to NetCDF file my_growing_file.nc\r\n     length of time vector: 0\r\n     empty file, starting from scratch\r\n    Sleeping...\r\n\r\n    Appending to NetCDF file my_growing_file.nc\r\n     length of time vector: 1\r\n     last time stamp in NetCDF: 2018-04-12 08:52:39.145999\r\n    Sleeping...\r\n\r\n    Appending to NetCDF file my_growing_file.nc\r\n     length of time vector: 2\r\n     last time stamp in NetCDF: 2018-04-12 08:52:42.159254\r\n    Sleeping...\r\n\r\n    Appending to NetCDF file my_growing_file.nc\r\n     length of time vector: 3\r\n     last time stamp in NetCDF: 2018-04-12 08:52:45.169516\r\n    Sleeping...\r\n\r\n    ---------------------------------------------------------------------------\r\n    IOError                                   Traceback (most recent call last)\r\n    &lt;ipython-input-17-9950ca2e53a6&gt; in &lt;module&gt;()\r\n         37 \r\n         38 for i in range(1000):\r\n    ---&gt; 39     with netCDF4.Dataset(fn, &#39;a&#39;) as nc_fh:\r\n         40         current_length = len(nc_fh[&#39;time&#39;])\r\n         41 \r\n\r\n    netCDF4/_netCDF4.pyx in netCDF4._netCDF4.Dataset.__init__()\r\n\r\n    netCDF4/_netCDF4.pyx in netCDF4._netCDF4._ensure_nc_success()\r\n\r\n    IOError: [Errno -101] NetCDF: HDF error: &#39;my_growing_file.nc&#39;\r\n\r\nIf using \r\n\r\n&lt;!-- language: python --&gt;\r\n\r\n    ds = xr.open_dataset(&#39;my_growing_file.nc&#39;, autoclose=True)\r\n\r\nthere is no error, but access times via `xarray` of course get slower, which is exactly my problem since my dashboard visualization gets very laggy.\r\n\r\nI can understand that this is maybe not the intended use for `xarray` and, if required, I will fall back to the lower level interface provided by `netCDF4` (hoping that it supports concurrent file access, at least for reads), but I would like to keep `xarray` for its convenience.",
            "link": "https://stackoverflow.com/questions/49701623/is-there-a-way-to-release-the-file-lock-for-a-xarray-dataset",
            "title": "Is there a way to release the file lock for a xarray.Dataset?",
            "body": "<p>I have a process that grows a NetCDF file <code>fn</code> every 5 minutes using <code>netcdf4.Dataset(fn, mode=a)</code>. I also have a bokeh server visualization of that NetCDF file using a <code>xarray.Dataset</code> (which I want to keep, because it is so convenient).</p>\n\n<p><strong>The problem</strong> is that the NetCDF-update-process fails when trying to add new data to <code>fn</code> if it is open in my bokeh server process via</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>ds = xarray.open_dataset(fn)\n</code></pre>\n\n<p>If I use the option <code>autoclose</code></p>\n\n<pre class=\"lang-py prettyprint-override\"><code>ds = xarray.open_dataset(fn, autoclose=True)\n</code></pre>\n\n<p>updating <code>fn</code> with the other process while <code>ds</code> is \"open\" in the bokeh server app works, but the updates to the bokeh figure, which pull time slices from <code>fn</code>, get very laggy.</p>\n\n<p><strong>My question is</strong>: Is there another way to release the lock of the NetCDF file when using <code>xarray.Dataset</code>? </p>\n\n<p>I would not care if the shape of the xarray.Dataset is only updated consistently after reloading the whole bokeh server app.</p>\n\n<p>Thanks!</p>\n\n<p><strong>Here is a minimal working example:</strong></p>\n\n<p>Put this into a file and let it run:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import time\nfrom datetime import datetime\n\nimport numpy as np\nimport netCDF4\n\nfn = 'my_growing_file.nc'\n\nwith netCDF4.Dataset(fn, 'w') as nc_fh:\n    # create dimensions\n    nc_fh.createDimension('x', 90)\n    nc_fh.createDimension('y', 90)\n    nc_fh.createDimension('time', None)\n\n    # create variables\n    nc_fh.createVariable('x', 'f8', ('x'))\n    nc_fh.createVariable('y', 'f8', ('y'))\n    nc_fh.createVariable('time', 'f8', ('time'))\n    nc_fh.createVariable('rainfall_amount',\n                         'i2',\n                         ('time', 'y', 'x'),\n                         zlib=False,\n                         complevel=0,\n                         fill_value=-9999,\n                         chunksizes=(1, 90, 90))\n    nc_fh['rainfall_amount'].scale_factor = 0.1\n    nc_fh['rainfall_amount'].add_offset = 0\n\n    nc_fh.set_auto_maskandscale(True)\n\n    # variable attributes\n    nc_fh['time'].long_name = 'Time'\n    nc_fh['time'].standard_name = 'time'\n    nc_fh['time'].units = 'hours since 2000-01-01 00:50:00.0'\n    nc_fh['time'].calendar = 'standard'\n\nfor i in range(1000):\n    with netCDF4.Dataset(fn, 'a') as nc_fh:\n        current_length = len(nc_fh['time'])\n\n        print('Appending to NetCDF file {}'.format(fn))\n        print(' length of time vector: {}'.format(current_length))\n\n        if current_length &gt; 0:\n            last_time_stamp = netCDF4.num2date(\n                nc_fh['time'][-1],\n                units=nc_fh['time'].units,\n                calendar=nc_fh['time'].calendar)\n            print(' last time stamp in NetCDF: {}'.format(str(last_time_stamp)))\n        else:\n            last_time_stamp = '1900-01-01'\n            print(' empty file, starting from scratch')\n\n        nc_fh['time'][i] = netCDF4.date2num(\n            datetime.utcnow(),\n            units=nc_fh['time'].units,\n            calendar=nc_fh['time'].calendar)\n        nc_fh['rainfall_amount'][i, :, :] = np.random.rand(90, 90)\n\n    print('Sleeping...\\n')\n    time.sleep(3)\n</code></pre>\n\n<p>Then, go to e.g. IPython and open the growing file via:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>ds = xr.open_dataset('my_growing_file.nc')\n</code></pre>\n\n<p>This will cause the process that appends to the NetCDF to fail with an output like this:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>Appending to NetCDF file my_growing_file.nc\n length of time vector: 0\n empty file, starting from scratch\nSleeping...\n\nAppending to NetCDF file my_growing_file.nc\n length of time vector: 1\n last time stamp in NetCDF: 2018-04-12 08:52:39.145999\nSleeping...\n\nAppending to NetCDF file my_growing_file.nc\n length of time vector: 2\n last time stamp in NetCDF: 2018-04-12 08:52:42.159254\nSleeping...\n\nAppending to NetCDF file my_growing_file.nc\n length of time vector: 3\n last time stamp in NetCDF: 2018-04-12 08:52:45.169516\nSleeping...\n\n---------------------------------------------------------------------------\nIOError                                   Traceback (most recent call last)\n&lt;ipython-input-17-9950ca2e53a6&gt; in &lt;module&gt;()\n     37 \n     38 for i in range(1000):\n---&gt; 39     with netCDF4.Dataset(fn, 'a') as nc_fh:\n     40         current_length = len(nc_fh['time'])\n     41 \n\nnetCDF4/_netCDF4.pyx in netCDF4._netCDF4.Dataset.__init__()\n\nnetCDF4/_netCDF4.pyx in netCDF4._netCDF4._ensure_nc_success()\n\nIOError: [Errno -101] NetCDF: HDF error: 'my_growing_file.nc'\n</code></pre>\n\n<p>If using </p>\n\n<pre class=\"lang-py prettyprint-override\"><code>ds = xr.open_dataset('my_growing_file.nc', autoclose=True)\n</code></pre>\n\n<p>there is no error, but access times via <code>xarray</code> of course get slower, which is exactly my problem since my dashboard visualization gets very laggy.</p>\n\n<p>I can understand that this is maybe not the intended use for <code>xarray</code> and, if required, I will fall back to the lower level interface provided by <code>netCDF4</code> (hoping that it supports concurrent file access, at least for reads), but I would like to keep <code>xarray</code> for its convenience.</p>\n"
        },
        {
            "tags": [
                "cordova-plugins"
            ],
            "owner": {
                "reputation": 8,
                "user_id": 9253318,
                "user_type": "registered",
                "profile_image": "https://i.stack.imgur.com/rhrN9.jpg?s=128&g=1",
                "display_name": "MOTZI",
                "link": "https://stackoverflow.com/users/9253318/motzi"
            },
            "is_answered": true,
            "view_count": 7,
            "accepted_answer_id": 49792484,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1523524184,
            "creation_date": 1519738658,
            "question_id": 49010021,
            "body_markdown": "I created a custom cordova plugin that needs a resource file to work, a *.zip* file and an image file to be precise *(.jpg)*.\r\n\r\n**Firstly i don&#39;t know how and where to add them properly in the plugin.xml** *(do i need to add them with the `&lt;source-file&gt;` , `&lt;resource-file&gt;` or if there is another tag that i should use)*.\r\n\r\n**Secondly i need to know the path of their location**, to use them in my code.\r\n\r\nAn example would be very helpful, thanks in advance.",
            "link": "https://stackoverflow.com/questions/49010021/creating-custom-cordova-plugin-that-needs-resources-zip-jpg",
            "title": "Creating custom Cordova plugin that needs resources (.zip /.jpg)",
            "body": "<p>I created a custom cordova plugin that needs a resource file to work, a <em>.zip</em> file and an image file to be precise <em>(.jpg)</em>.</p>\n\n<p><strong>Firstly i don't know how and where to add them properly in the plugin.xml</strong> <em>(do i need to add them with the <code>&lt;source-file&gt;</code> , <code>&lt;resource-file&gt;</code> or if there is another tag that i should use)</em>.</p>\n\n<p><strong>Secondly i need to know the path of their location</strong>, to use them in my code.</p>\n\n<p>An example would be very helpful, thanks in advance.</p>\n"
        },
        {
            "tags": [
                "java",
                "osgi",
                "bndtools"
            ],
            "owner": {
                "reputation": 166,
                "user_id": 5284104,
                "user_type": "registered",
                "accept_rate": 100,
                "profile_image": "https://graph.facebook.com/10207652878070610/picture?type=large",
                "display_name": "Robert Koszewski",
                "link": "https://stackoverflow.com/users/5284104/robert-koszewski"
            },
            "is_answered": true,
            "view_count": 80,
            "accepted_answer_id": 47632916,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1523524183,
            "creation_date": 1512386152,
            "last_edit_date": 1512387133,
            "question_id": 47632109,
            "body_markdown": "I&#39;m working on this BndTools based OSGi project: https://github.com/Jafre13/ISS-Product\r\n\r\nMy project though requires a non-OSGi based library (aliasi-lingpipe - https://mvnrepository.com/artifact/de.julielab/aliasi-lingpipe/4.1.0). \r\n\r\nAnd here is where the problem starts. First I tried to add the JAR directly from the Maven repository, as BndTools is supposedly compatible with Maven repositories, but it doesn&#39;t seem to work from the &quot;Repository Browser&quot;. Even when adding the Maven Central plugin to the build.bnd file as stated in http://bnd.bndtools.org/plugins/maven.html, still yields the same result.\r\n\r\nGiving up on having Maven working, I tried to do it the most spartan way and download the JAR myself and add it as plain JAR as specified here: http://bndtools.org/faq.html (6 How Can I Depend on a Plain JAR File at Build Time?)\r\n\r\nThis time the JAR got recognised and I could start using it in the code. But once you launch the OSGi framework everything just crumbles again as it is trying to resolve the non-OSGi class paths, resulting in a wiring error like this:\r\n\r\n&gt; could not resolve the bundles: [slf4j.api-1.7.25\r\n&gt; org.osgi.framework.BundleException: Unable to resolve slf4j.api [6](R\r\n&gt; 6.0): missing requirement [slf4j.api [6](R 6.0)] osgi.wiring.package; (&amp;(osgi.wiring.package=org.slf4j.impl)(version&gt;=1.6.0)) Unresolved\r\n&gt; requirements: [[slf4j.api [6](R 6.0)] osgi.wiring.package;\r\n&gt; (&amp;(osgi.wiring.package=org.slf4j.impl)(version&gt;=1.6.0))] ,\r\n&gt; dk.sdu.sso.sred-0.0.0.201712041036 org.osgi.framework.BundleException:\r\n&gt; Unable to resolve dk.sdu.sso.sred [7](R 7.0): missing requirement\r\n&gt; [dk.sdu.sso.sred [7](R 7.0)] osgi.wiring.package;\r\n&gt; (osgi.wiring.package=com.aliasi.classify) Unresolved requirements:\r\n&gt; [[dk.sdu.sso.sred [7](R 7.0)] osgi.wiring.package;\r\n&gt; (osgi.wiring.package=com.aliasi.classify)] ]\r\n\r\nSo at this point I&#39;m completely blocked and unable to proceed. I hope somebody with some OSGi experience could help here.\r\n\r\nGreetings and thanks.",
            "link": "https://stackoverflow.com/questions/47632109/bndtools-how-to-add-a-non-osgi-jar-using-plain-jar-breaks-the-whole-project-m",
            "title": "BndTools, How to add a non-OSGi JAR? Using plain JAR breaks the whole project. Missing requirement wiring package",
            "body": "<p>I'm working on this BndTools based OSGi project: <a href=\"https://github.com/Jafre13/ISS-Product\" rel=\"nofollow noreferrer\">https://github.com/Jafre13/ISS-Product</a></p>\n\n<p>My project though requires a non-OSGi based library (aliasi-lingpipe - <a href=\"https://mvnrepository.com/artifact/de.julielab/aliasi-lingpipe/4.1.0\" rel=\"nofollow noreferrer\">https://mvnrepository.com/artifact/de.julielab/aliasi-lingpipe/4.1.0</a>). </p>\n\n<p>And here is where the problem starts. First I tried to add the JAR directly from the Maven repository, as BndTools is supposedly compatible with Maven repositories, but it doesn't seem to work from the \"Repository Browser\". Even when adding the Maven Central plugin to the build.bnd file as stated in <a href=\"http://bnd.bndtools.org/plugins/maven.html\" rel=\"nofollow noreferrer\">http://bnd.bndtools.org/plugins/maven.html</a>, still yields the same result.</p>\n\n<p>Giving up on having Maven working, I tried to do it the most spartan way and download the JAR myself and add it as plain JAR as specified here: <a href=\"http://bndtools.org/faq.html\" rel=\"nofollow noreferrer\">http://bndtools.org/faq.html</a> (6 How Can I Depend on a Plain JAR File at Build Time?)</p>\n\n<p>This time the JAR got recognised and I could start using it in the code. But once you launch the OSGi framework everything just crumbles again as it is trying to resolve the non-OSGi class paths, resulting in a wiring error like this:</p>\n\n<blockquote>\n  <p>could not resolve the bundles: [slf4j.api-1.7.25\n  org.osgi.framework.BundleException: Unable to resolve slf4j.api [6](R\n  6.0): missing requirement [slf4j.api [6](R 6.0)] osgi.wiring.package; (&amp;(osgi.wiring.package=org.slf4j.impl)(version>=1.6.0)) Unresolved\n  requirements: [[slf4j.api [6](R 6.0)] osgi.wiring.package;\n  (&amp;(osgi.wiring.package=org.slf4j.impl)(version>=1.6.0))] ,\n  dk.sdu.sso.sred-0.0.0.201712041036 org.osgi.framework.BundleException:\n  Unable to resolve dk.sdu.sso.sred [7](R 7.0): missing requirement\n  [dk.sdu.sso.sred [7](R 7.0)] osgi.wiring.package;\n  (osgi.wiring.package=com.aliasi.classify) Unresolved requirements:\n  [[dk.sdu.sso.sred [7](R 7.0)] osgi.wiring.package;\n  (osgi.wiring.package=com.aliasi.classify)] ]</p>\n</blockquote>\n\n<p>So at this point I'm completely blocked and unable to proceed. I hope somebody with some OSGi experience could help here.</p>\n\n<p>Greetings and thanks.</p>\n"
        },
        {
            "tags": [
                "api",
                "tfs"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 9586800,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/ab17c672bfe38643dc7ea29eaf1059f2?s=128&d=identicon&r=PG&f=1",
                "display_name": "Thou Curator",
                "link": "https://stackoverflow.com/users/9586800/thou-curator"
            },
            "is_answered": false,
            "view_count": 35,
            "answer_count": 3,
            "score": -5,
            "last_activity_date": 1523524181,
            "creation_date": 1522683259,
            "last_edit_date": 1522691064,
            "question_id": 49614112,
            "body_markdown": "Hello can anyone tell me how to delete files using the API for TFS?  Below is what I have but I can not get it to work any help would really be appreciated.\r\n\r\n    string[] InLocalDirectory = Directory.GetFiles(LogicAppConfig.Query(AppConfigLogic.TypeOfConfig.Path), &quot;*&quot;, SearchOption.AllDirectories);\r\n\r\n    // Source Control\r\n    List&lt;string&gt; InSourceControl = new List&lt;string&gt;();\r\n    ItemSet SetOfItem = _ServerVersionControl.GetItems(_ServerPath, VersionSpec.Latest, RecursionType.Full);\r\n    foreach (Item GotItem in SetOfItem.Items)\r\n    {\r\n        ItemType TypeOfItem = GotItem.ItemType;\r\n        if (TypeOfItem == ItemType.File)\r\n        {\r\n            string LocalPath = _WorkspaceLocal.GetLocalItemForServerItem(GotItem.ServerItem);\r\n            InSourceControl.Add(LocalPath);\r\n        }\r\n    }\r\n\r\n    List&lt;int&gt; ToDeleteById = new List&lt;int&gt;();\r\n    foreach (string SourceFile in InSourceControl)\r\n    {\r\n        if (!IsIgnored(SourceFile) &amp;&amp; !InLocalDirectory.Contains(SourceFile))\r\n        {\r\n            // Delete Source Control File\r\n            Item DeleteItem = _ServerVersionControl.GetItem(SourceFile);\r\n            ToDeleteById.Add(DeleteItem.ItemId);\r\n            // Update Local XML Directory\r\n            DataXml.Delete(SourceFile);\r\n        }\r\n    }\r\n\r\n    WorkItemStore wis = _CollectionTeamProject.GetService&lt;WorkItemStore&gt;();\r\n    wis.DestroyWorkItems(ToDeleteById);",
            "link": "https://stackoverflow.com/questions/49614112/issue-with-api-deleting-for-team-foundation-server-tfs",
            "title": "Issue with API Deleting for Team Foundation Server TFS",
            "body": "<p>Hello can anyone tell me how to delete files using the API for TFS?  Below is what I have but I can not get it to work any help would really be appreciated.</p>\n\n<pre><code>string[] InLocalDirectory = Directory.GetFiles(LogicAppConfig.Query(AppConfigLogic.TypeOfConfig.Path), \"*\", SearchOption.AllDirectories);\n\n// Source Control\nList&lt;string&gt; InSourceControl = new List&lt;string&gt;();\nItemSet SetOfItem = _ServerVersionControl.GetItems(_ServerPath, VersionSpec.Latest, RecursionType.Full);\nforeach (Item GotItem in SetOfItem.Items)\n{\n    ItemType TypeOfItem = GotItem.ItemType;\n    if (TypeOfItem == ItemType.File)\n    {\n        string LocalPath = _WorkspaceLocal.GetLocalItemForServerItem(GotItem.ServerItem);\n        InSourceControl.Add(LocalPath);\n    }\n}\n\nList&lt;int&gt; ToDeleteById = new List&lt;int&gt;();\nforeach (string SourceFile in InSourceControl)\n{\n    if (!IsIgnored(SourceFile) &amp;&amp; !InLocalDirectory.Contains(SourceFile))\n    {\n        // Delete Source Control File\n        Item DeleteItem = _ServerVersionControl.GetItem(SourceFile);\n        ToDeleteById.Add(DeleteItem.ItemId);\n        // Update Local XML Directory\n        DataXml.Delete(SourceFile);\n    }\n}\n\nWorkItemStore wis = _CollectionTeamProject.GetService&lt;WorkItemStore&gt;();\nwis.DestroyWorkItems(ToDeleteById);\n</code></pre>\n"
        },
        {
            "tags": [
                "email"
            ],
            "owner": {
                "reputation": 12910,
                "user_id": 465233,
                "user_type": "registered",
                "accept_rate": 80,
                "profile_image": "https://www.gravatar.com/avatar/05651f9274b9713888a4bd6864221c36?s=128&d=identicon&r=PG",
                "display_name": "MatTheCat",
                "link": "https://stackoverflow.com/users/465233/matthecat"
            },
            "is_answered": false,
            "view_count": 9,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1523524178,
            "creation_date": 1523524178,
            "question_id": 49792481,
            "body_markdown": "I posted the same question on [webapps][1] but I&#39;m not sure where this belongs so here it is:\r\n\r\nLet&#39;s say I log into a website with my email address somewords@gmail.com. I cannot prevent someone to use the same just using a unique index because Some.Words@gmail.com would still be the same as per GMail rules. The soution would be to compute a canonical email and make it unique, but how to compute it?\r\n\r\nI couldn&#39;t find any resource on this subject apart concerning GMail. Is it because it&#39;s the only mailbox provider doing this? If not are there general rules? If not what are provider&#39;s specific rules?\r\n\r\n\r\n  [1]: https://webapps.stackexchange.com/questions/116242/how-to-compute-a-canonical-email",
            "link": "https://stackoverflow.com/questions/49792481/how-to-compute-a-canonical-email",
            "title": "How to compute a canonical email?",
            "body": "<p>I posted the same question on <a href=\"https://webapps.stackexchange.com/questions/116242/how-to-compute-a-canonical-email\">webapps</a> but I'm not sure where this belongs so here it is:</p>\n\n<p>Let's say I log into a website with my email address somewords@gmail.com. I cannot prevent someone to use the same just using a unique index because Some.Words@gmail.com would still be the same as per GMail rules. The soution would be to compute a canonical email and make it unique, but how to compute it?</p>\n\n<p>I couldn't find any resource on this subject apart concerning GMail. Is it because it's the only mailbox provider doing this? If not are there general rules? If not what are provider's specific rules?</p>\n"
        },
        {
            "tags": [
                "javascript",
                "arrays",
                "nested",
                "reduce"
            ],
            "owner": {
                "reputation": 44,
                "user_id": 9371234,
                "user_type": "registered",
                "accept_rate": 75,
                "profile_image": "https://www.gravatar.com/avatar/8496f53b64cdb810618a859a65323c41?s=128&d=identicon&r=PG&f=1",
                "display_name": "bax",
                "link": "https://stackoverflow.com/users/9371234/bax"
            },
            "is_answered": true,
            "view_count": 29,
            "answer_count": 3,
            "score": 0,
            "last_activity_date": 1523524169,
            "creation_date": 1523517172,
            "question_id": 49790202,
            "body_markdown": "I&#39;m trying to complete this tutorial, but I cannot figure out why the answer the instructor gave is not working for me. \r\n\r\nThere is an array of users (objects) that each have favoriteBooks as nested objects. \r\n\r\nWe are to use map and reduce to create an array of all favorite books from all users. \r\n\r\nThe code I have on the bottom is what I have so far, but is also what the instructor said the solution is. His code worked to create the exact solution (on his video).\r\n\r\nHowever, I think the problem is when the second map comes into play, there is still another level of nested arrays to get through. \r\n\r\nCan someone please shed some light on this? I cannot figure it out for the life of me. \r\n\r\nThanks!\r\n\r\n\r\n\r\n    const users = [\r\n      {\r\n        name: &#39;Samir&#39;,\r\n        age: 27,\r\n        favoriteBooks:[\r\n          {title: &#39;The Iliad&#39;},\r\n          {title: &#39;The Brothers Karamazov&#39;}\r\n        ]\r\n      },\r\n      {\r\n        name: &#39;Angela&#39;,\r\n        age: 33,\r\n        favoriteBooks:[\r\n          {title: &#39;Tenth of December&#39;},\r\n          {title: &#39;Cloud Atlas&#39;},\r\n          {title: &#39;One Hundred Years of Solitude&#39;}\r\n        ]\r\n      },\r\n      {\r\n        name: &#39;Beatrice&#39;,\r\n        age: 42,\r\n        favoriteBooks:[\r\n          {title: &#39;Candide&#39;}\r\n        ]\r\n      }\r\n    ];\r\n    \r\n        // Result: [&#39;The Iliad&#39;, &#39;The Brothers Karamazov&#39;, &#39;Tenth of December&#39;, &#39;Cloud Atlas&#39;, &#39;One Hundred Years of Solitude&#39;, &#39;Candide&#39;];\r\n        \r\n    var books = users\r\n      .map(function(user) {\r\n        return user.favoriteBooks;\r\n      })\r\n      .map(function(book) {\r\n        return book.title;\r\n      })\r\n      .reduce(function(arr, titles) {\r\n        return [ ...arr, ...titles ];\r\n      }, []);\r\n      \r\n    console.log(books);",
            "link": "https://stackoverflow.com/questions/49790202/javascript-tutorial-having-trouble-mapping-reducing-to-get-correct-answer",
            "title": "Javascript tutorial, having trouble mapping &amp; reducing to get correct answer",
            "body": "<p>I'm trying to complete this tutorial, but I cannot figure out why the answer the instructor gave is not working for me. </p>\n\n<p>There is an array of users (objects) that each have favoriteBooks as nested objects. </p>\n\n<p>We are to use map and reduce to create an array of all favorite books from all users. </p>\n\n<p>The code I have on the bottom is what I have so far, but is also what the instructor said the solution is. His code worked to create the exact solution (on his video).</p>\n\n<p>However, I think the problem is when the second map comes into play, there is still another level of nested arrays to get through. </p>\n\n<p>Can someone please shed some light on this? I cannot figure it out for the life of me. </p>\n\n<p>Thanks!</p>\n\n<pre><code>const users = [\n  {\n    name: 'Samir',\n    age: 27,\n    favoriteBooks:[\n      {title: 'The Iliad'},\n      {title: 'The Brothers Karamazov'}\n    ]\n  },\n  {\n    name: 'Angela',\n    age: 33,\n    favoriteBooks:[\n      {title: 'Tenth of December'},\n      {title: 'Cloud Atlas'},\n      {title: 'One Hundred Years of Solitude'}\n    ]\n  },\n  {\n    name: 'Beatrice',\n    age: 42,\n    favoriteBooks:[\n      {title: 'Candide'}\n    ]\n  }\n];\n\n    // Result: ['The Iliad', 'The Brothers Karamazov', 'Tenth of December', 'Cloud Atlas', 'One Hundred Years of Solitude', 'Candide'];\n\nvar books = users\n  .map(function(user) {\n    return user.favoriteBooks;\n  })\n  .map(function(book) {\n    return book.title;\n  })\n  .reduce(function(arr, titles) {\n    return [ ...arr, ...titles ];\n  }, []);\n\nconsole.log(books);\n</code></pre>\n"
        },
        {
            "tags": [
                "python",
                "pandas",
                "append",
                "concat"
            ],
            "owner": {
                "reputation": 16,
                "user_id": 9532692,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/e87cafe12e6ad3962caaab0743a64b3c?s=128&d=identicon&r=PG&f=1",
                "display_name": "user9532692",
                "link": "https://stackoverflow.com/users/9532692/user9532692"
            },
            "is_answered": true,
            "view_count": 26,
            "accepted_answer_id": 49792125,
            "answer_count": 2,
            "score": 1,
            "last_activity_date": 1523524165,
            "creation_date": 1523515961,
            "question_id": 49789862,
            "body_markdown": "I am trying to concatenate three separate dataframes (bs_df, income_df, cash_df) of each ticker then append them into a dataframe **df**. It successfully appends the dataframe the first time it goes through the for loop, but when it tries to append the new dataframe to the original dataframe, it fails to do so and throws an **Assertion Error**. I think it&#39;s because the dataframes have different columns ([reference][1]), but I cannot figure out a way to bypass this and still create a dataframe with all the column names and fill in with NaN values if a dataframe does not have the value for a specific column that the other dataframe has.\r\n\r\nHere is the code I am running: \r\n\r\n    from yahoofinancials import YahooFinancials\r\n    import pandas as pd\r\n    \r\n    tickers = [&quot;PIH&quot;,&quot;AAPL&quot;,&quot;AMZN&quot;]\r\n    df = pd.DataFrame()\r\n    for ticker in tickers:\r\n        print (ticker)\r\n        # Updates every loop\r\n        ticker_yahoo_financials = YahooFinancials(ticker)\r\n        \r\n        # Get financial info\r\n        bs_hist = ticker_yahoo_financials.get_financial_stmts(&#39;annual&#39;, &#39;balance&#39;)\r\n        income_hist = ticker_yahoo_financials.get_financial_stmts(&#39;annual&#39;, &#39;income&#39;)\r\n        cash_hist = ticker_yahoo_financials.get_financial_stmts(&#39;annual&#39;, &#39;cash&#39;)\r\n        \r\n        # Create dataframes and comebine them\r\n        bs_df = pd.DataFrame(list(bs_hist[&#39;balanceSheetHistory&#39;][ticker][0].values()))\r\n        income_df = pd.DataFrame(list(income_hist[&#39;incomeStatementHistory&#39;][ticker][0].values()))\r\n        cash_df = pd.DataFrame(list(cash_hist[&#39;cashflowStatementHistory&#39;][ticker][0].values()))\r\n        comb = pd.concat([bs_df, income_df, cash_df], axis=1)\r\n        df = df.append(comb)\r\n\r\nHere is the error message:\r\n\r\n    ---------------------------------------------------------------------------\r\n    AssertionError                            Traceback (most recent call last)\r\n    &lt;ipython-input-23-f476ca9fbb70&gt; in &lt;module&gt;()\r\n         19     cash_df = pd.DataFrame(list(cash_hist[&#39;cashflowStatementHistory&#39;][ticker][0].values()))\r\n         20     comb = pd.concat([bs_df, income_df, cash_df], axis=1)\r\n    ---&gt; 21     df = df.append(comb)\r\n    \r\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py in append(self, other, ignore_index, verify_integrity)\r\n       5192             to_concat = [self, other]\r\n       5193         return concat(to_concat, ignore_index=ignore_index,\r\n    -&gt; 5194                       verify_integrity=verify_integrity)\r\n       5195 \r\n       5196     def join(self, other, on=None, how=&#39;left&#39;, lsuffix=&#39;&#39;, rsuffix=&#39;&#39;,\r\n    \r\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\r\n        211                        verify_integrity=verify_integrity,\r\n        212                        copy=copy)\r\n    --&gt; 213     return op.get_result()\r\n        214 \r\n        215 \r\n    \r\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py in get_result(self)\r\n        406             new_data = concatenate_block_managers(\r\n        407                 mgrs_indexers, self.new_axes, concat_axis=self.axis,\r\n    --&gt; 408                 copy=self.copy)\r\n        409             if not self.copy:\r\n        410                 new_data._consolidate_inplace()\r\n    \r\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py in concatenate_block_managers(mgrs_indexers, axes, concat_axis, copy)\r\n       5205         blocks.append(b)\r\n       5206 \r\n    -&gt; 5207     return BlockManager(blocks, axes)\r\n       5208 \r\n       5209 \r\n    \r\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py in __init__(self, blocks, axes, do_integrity_check, fastpath)\r\n       3031 \r\n       3032         if do_integrity_check:\r\n    -&gt; 3033             self._verify_integrity()\r\n       3034 \r\n       3035         self._consolidate_check()\r\n    \r\n    C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py in _verify_integrity(self)\r\n       3247                                  &#39;block items\\n# manager items: {0}, # &#39;\r\n       3248                                  &#39;tot_items: {1}&#39;.format(\r\n    -&gt; 3249                                      len(self.items), tot_items))\r\n       3250 \r\n       3251     def apply(self, f, axes=None, filter=None, do_integrity_check=False,\r\n    \r\n    AssertionError: Number of manager items must equal union of block items\r\n    # manager items: 67, # tot_items: 68\r\n\r\n  [1]: https://stackoverflow.com/a/39616022",
            "link": "https://stackoverflow.com/questions/49789862/python-pandas-append-dataframe-failing",
            "title": "python - pandas append dataframe failing",
            "body": "<p>I am trying to concatenate three separate dataframes (bs_df, income_df, cash_df) of each ticker then append them into a dataframe <strong>df</strong>. It successfully appends the dataframe the first time it goes through the for loop, but when it tries to append the new dataframe to the original dataframe, it fails to do so and throws an <strong>Assertion Error</strong>. I think it's because the dataframes have different columns (<a href=\"https://stackoverflow.com/a/39616022\">reference</a>), but I cannot figure out a way to bypass this and still create a dataframe with all the column names and fill in with NaN values if a dataframe does not have the value for a specific column that the other dataframe has.</p>\n\n<p>Here is the code I am running: </p>\n\n<pre><code>from yahoofinancials import YahooFinancials\nimport pandas as pd\n\ntickers = [\"PIH\",\"AAPL\",\"AMZN\"]\ndf = pd.DataFrame()\nfor ticker in tickers:\n    print (ticker)\n    # Updates every loop\n    ticker_yahoo_financials = YahooFinancials(ticker)\n\n    # Get financial info\n    bs_hist = ticker_yahoo_financials.get_financial_stmts('annual', 'balance')\n    income_hist = ticker_yahoo_financials.get_financial_stmts('annual', 'income')\n    cash_hist = ticker_yahoo_financials.get_financial_stmts('annual', 'cash')\n\n    # Create dataframes and comebine them\n    bs_df = pd.DataFrame(list(bs_hist['balanceSheetHistory'][ticker][0].values()))\n    income_df = pd.DataFrame(list(income_hist['incomeStatementHistory'][ticker][0].values()))\n    cash_df = pd.DataFrame(list(cash_hist['cashflowStatementHistory'][ticker][0].values()))\n    comb = pd.concat([bs_df, income_df, cash_df], axis=1)\n    df = df.append(comb)\n</code></pre>\n\n<p>Here is the error message:</p>\n\n<pre><code>---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\n&lt;ipython-input-23-f476ca9fbb70&gt; in &lt;module&gt;()\n     19     cash_df = pd.DataFrame(list(cash_hist['cashflowStatementHistory'][ticker][0].values()))\n     20     comb = pd.concat([bs_df, income_df, cash_df], axis=1)\n---&gt; 21     df = df.append(comb)\n\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py in append(self, other, ignore_index, verify_integrity)\n   5192             to_concat = [self, other]\n   5193         return concat(to_concat, ignore_index=ignore_index,\n-&gt; 5194                       verify_integrity=verify_integrity)\n   5195 \n   5196     def join(self, other, on=None, how='left', lsuffix='', rsuffix='',\n\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\n    211                        verify_integrity=verify_integrity,\n    212                        copy=copy)\n--&gt; 213     return op.get_result()\n    214 \n    215 \n\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py in get_result(self)\n    406             new_data = concatenate_block_managers(\n    407                 mgrs_indexers, self.new_axes, concat_axis=self.axis,\n--&gt; 408                 copy=self.copy)\n    409             if not self.copy:\n    410                 new_data._consolidate_inplace()\n\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py in concatenate_block_managers(mgrs_indexers, axes, concat_axis, copy)\n   5205         blocks.append(b)\n   5206 \n-&gt; 5207     return BlockManager(blocks, axes)\n   5208 \n   5209 \n\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py in __init__(self, blocks, axes, do_integrity_check, fastpath)\n   3031 \n   3032         if do_integrity_check:\n-&gt; 3033             self._verify_integrity()\n   3034 \n   3035         self._consolidate_check()\n\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py in _verify_integrity(self)\n   3247                                  'block items\\n# manager items: {0}, # '\n   3248                                  'tot_items: {1}'.format(\n-&gt; 3249                                      len(self.items), tot_items))\n   3250 \n   3251     def apply(self, f, axes=None, filter=None, do_integrity_check=False,\n\nAssertionError: Number of manager items must equal union of block items\n# manager items: 67, # tot_items: 68\n</code></pre>\n"
        },
        {
            "tags": [
                "python",
                "python-3.x"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 8510156,
                "user_type": "registered",
                "profile_image": "https://lh6.googleusercontent.com/-i6bUvgLvRH0/AAAAAAAAAAI/AAAAAAAAABs/EZ7UeC4nRBo/photo.jpg?sz=128",
                "display_name": "Sikandar Ali",
                "link": "https://stackoverflow.com/users/8510156/sikandar-ali"
            },
            "is_answered": false,
            "view_count": 16,
            "answer_count": 0,
            "score": -1,
            "last_activity_date": 1523524161,
            "creation_date": 1523523777,
            "last_edit_date": 1523524161,
            "question_id": 49792343,
            "body_markdown": "    train = [(&#39;I love this sandwich.&#39;,&#39;pos&#39;),\r\n         (&#39;This is an amazing place!&#39;, &#39;pos&#39;),\r\n         (&#39;I feel very good about these beers.&#39;, &#39;pos&#39;),\r\n         (&#39;This is my best work.&#39;, &#39;pos&#39;),\r\n         (&#39;What an awesome view&#39;, &#39;pos&#39;),\r\n         (&#39;I do not like this restaurant&#39;, &#39;neg&#39;),\r\n         (&#39;I am tired of this stuff.&#39;, &#39;neg&#39;),\r\n         (&quot;I can&#39;t deal with this.&quot;, &#39;neg&#39;),\r\n         (&#39;He is my sworn enemy!.&#39;, &#39;neg&#39;),\r\n         (&#39;My boss is horrible.&#39;, &#39;neg&#39;)\r\n        ]`\r\nThis is my training data\r\nNow after using count vectoizer and tfidf transformer to change the data in matrix form. Now how can i train my classifier MultinomialNB?\r\nPlease help me out...Thank you\r\n    [This is countVetorizer ][1]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/2tNN1.png",
            "link": "https://stackoverflow.com/questions/49792343/how-can-i-train-my-classifier-after-countvectorizer-and-tfidf-transformer",
            "title": "How can i train my classifier after countvectorizer and tfidf transformer?",
            "body": "<pre><code>train = [('I love this sandwich.','pos'),\n     ('This is an amazing place!', 'pos'),\n     ('I feel very good about these beers.', 'pos'),\n     ('This is my best work.', 'pos'),\n     ('What an awesome view', 'pos'),\n     ('I do not like this restaurant', 'neg'),\n     ('I am tired of this stuff.', 'neg'),\n     (\"I can't deal with this.\", 'neg'),\n     ('He is my sworn enemy!.', 'neg'),\n     ('My boss is horrible.', 'neg')\n    ]`\n</code></pre>\n\n<p>This is my training data\nNow after using count vectoizer and tfidf transformer to change the data in matrix form. Now how can i train my classifier MultinomialNB?\nPlease help me out...Thank you\n    <a href=\"https://i.stack.imgur.com/2tNN1.png\" rel=\"nofollow noreferrer\">This is countVetorizer </a></p>\n"
        },
        {
            "tags": [
                "python",
                "multiprocessing",
                "counter"
            ],
            "owner": {
                "reputation": 13,
                "user_id": 4219457,
                "user_type": "registered",
                "profile_image": "https://lh5.googleusercontent.com/-7aeIi_p04CM/AAAAAAAAAAI/AAAAAAAAAE4/D1cEjanoXzM/photo.jpg?sz=128",
                "display_name": "Chris Hayes",
                "link": "https://stackoverflow.com/users/4219457/chris-hayes"
            },
            "is_answered": false,
            "view_count": 18,
            "answer_count": 2,
            "score": 1,
            "last_activity_date": 1523524159,
            "creation_date": 1523520753,
            "question_id": 49791271,
            "body_markdown": "I need the counter variable (list_counter) inside my &#39;scraper&#39; function to increment for each iteration through list1. \r\n\r\nThe problem is it&#39;s assigning a counter **to each individual process**. \r\n\r\nI want each process to simply increment the global list_counter at the end of the loop, not for each process to have its own counter.\r\n\r\nI tried passing the variable as an argument but couldn&#39;t get it to work that way either.\r\n\r\nWhat you guys think? Is it even possible to have a global counter work with multiple processes - specifically using pool, map, lock?\r\n\r\n\r\n\tfrom multiprocessing import Lock, Pool\r\n\tfrom time import sleep\r\n\tfrom bs4 import BeautifulSoup\r\n\timport re\r\n\timport requests\r\n\t \r\n\texceptions = []\r\n\tlock = Lock()\r\n\tlist_counter = 0\r\n\t \r\n\t \r\n\tdef scraper(url):  # url is tied to the individual list items\r\n\t\t&quot;&quot;&quot;\r\n\t\tTesting multiprocessing and requests\r\n\t\t&quot;&quot;&quot;\r\n\t \r\n\t\tglobal list_counter\r\n\t \r\n\t\tlock.acquire()\r\n\t \r\n\t\ttry:\r\n\t \r\n\t\t\tscrape = requests.get(url,\r\n\t\t\t\t\t\t\t\t  headers={&quot;user-agent&quot;: &quot;Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36&quot;},\r\n\t\t\t\t\t\t\t\t  timeout=10)\r\n\t \r\n\t\t\tif scrape.status_code == 200:\r\n\t \r\n\t\t\t\t&quot;&quot;&quot; --------------------------------------------- &quot;&quot;&quot;\r\n\t\t\t\t# ---------------------------------------------------\r\n\t\t\t\t&#39;&#39;&#39;           --&gt; SCRAPE ALEXA RANK: &lt;--          &#39;&#39;&#39;\r\n\t\t\t\t# ---------------------------------------------------\r\n\t\t\t\t&quot;&quot;&quot; --------------------------------------------- &quot;&quot;&quot;\r\n\t \r\n\t\t\t\tsleep(0.1)\r\n\t\t\t\tscrape = requests.get(&quot;http://data.alexa.com/data?cli=10&amp;dat=s&amp;url=&quot; + url,\r\n\t\t\t\t\t\t\t\t\t  headers={&quot;user-agent&quot;: &quot;Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36&quot;})\r\n\t\t\t\thtml = scrape.content\r\n\t\t\t\tsoup = BeautifulSoup(html, &#39;lxml&#39;)\r\n\t \r\n\t\t\t\trank = re.findall(r&#39;&lt;popularity[^&gt;]*text=&quot;(\\d+)&quot;&#39;, str(soup))\r\n\t \r\n\t\t\t\tprint(&quot;Server Status:&quot;, scrape.status_code, &#39;-&#39;, u&quot;\\u2713&quot;, &#39;-&#39;, list_counter, &#39;-&#39;, url, &#39;-&#39;, &quot;Rank:&quot;, rank[0])\r\n\t \r\n\t\t\t\tlist_counter = list_counter + 1\r\n\t \r\n\t\t\telse:\r\n\t\t\t\tprint(&quot;Server Status:&quot;, scrape.status_code)\r\n\t\t\t\tlist_counter = list_counter + 1\r\n\t\t\t\tprint(list_counter)\r\n\t\t\t\tpass\r\n\t \r\n\t\texcept BaseException as e:\r\n\t\t\texceptions.append(e)\r\n\t\t\tprint()\r\n\t\t\tprint(e)\r\n\t\t\tprint()\r\n\t\t\tlist_counter = list_counter + 1\r\n\t\t\tprint(list_counter)\r\n\t\t\tpass\r\n\t \r\n\t\tfinally:\r\n\t\t\tlock.release()\r\n\t \r\n\tif __name__ == &#39;__main__&#39;:\r\n\t \r\n\t\tlist1 = [&quot;http://www.wallstreetinvestorplace.com/2018/04/cvs-health-corporation-cvs-to-touch-7-54-earnings-growth-for-next-year/&quot;,\r\n\t\t\t\t &quot;https://macondaily.com/2018/04/06/cetera-advisors-llc-lowers-position-in-cvs-health-cvs.html&quot;,\r\n\t\t\t\t &quot;http://www.thesportsbank.net/football/liverpool/jurgen-klopp-very-positive-about-mo-salah-injury/&quot;,\r\n\t\t\t\t &quot;https://www.moneyjournals.com/trump-wasting-time-trying-bring-amazon/&quot;,\r\n\t\t\t\t &quot;https://www.pmnewsnigeria.com/2018/04/06/fcta-targets-800000-children-for-polio-immunisation/&quot;,\r\n\t\t\t\t &quot;http://toronto.citynews.ca/2018/04/06/officials-in-canada-braced-for-another-spike-in-illegal-border-crossings/&quot;,\r\n\t\t\t\t &quot;https://www.pmnewsnigeria.com/2018/04/04/pdp-describes-looters-list-as-plot-to-divert-attention/&quot;,\r\n\t\t\t\t &quot;https://beyondpesticides.org/dailynewsblog/2018/04/epa-administrator-pruitt-colluding-regulated-industry/&quot;,\r\n\t\t\t\t &quot;http://thyblackman.com/2018/04/06/robert-mueller-is-searching-for/&quot;,\r\n\t\t\t\t &quot;https://www.theroar.com.au/2018/04/06/2018-commonwealth-games-swimming-night-2-finals-live-updates-results-blog/&quot;,\r\n\t\t\t\t &quot;https://medicalresearch.com/pain-research/migraine-linked-to-increased-risk-of-heart-disease-and-stroke/40858/&quot;,\r\n\t\t\t\t &quot;http://www.investingbizz.com/2018/04/amazon-com-inc-amzn-stock-creates-investors-concerns/&quot;,\r\n\t\t\t\t &quot;https://stocknewstimes.com/2018/04/06/convergence-investment-partners-llc-grows-position-in-amazon-com-inc-amzn.html&quot;,\r\n\t\t\t\t &quot;https://factsherald.com/old-food-rules-needs-to-be-updated/&quot;,\r\n\t\t\t\t &quot;https://www.nextadvisor.com/blog/2018/04/06/the-facebook-scandal-evolves/&quot;,\r\n\t\t\t\t &quot;http://sacramento.cbslocal.com/2018/04/04/police-family-youtube-shooter/&quot;,\r\n\t\t\t\t &quot;http://en.brinkwire.com/245768/why-does-stress-lead-to-weight-gain-study-sheds-light/&quot;,\r\n\t\t\t\t &quot;https://www.marijuana.com/news/2018/04/monterey-bud-jeff-sessions-is-on-the-wrong-side-of-history-science-and-public-opinion/&quot;,\r\n\t\t\t\t &quot;http://www.stocksgallery.com/2018/04/06/jpmorgan-chase-co-jpm-noted-a-price-change-of-0-80-and-amazon-com-inc-amzn-closes-with-a-move-of-2-92/&quot;,\r\n\t\t\t\t &quot;https://stocknewstimes.com/2018/04/06/front-barnett-associates-llc-has-2-41-million-position-in-cvs-health-corp-cvs.html&quot;,\r\n\t\t\t\t &quot;http://www.liveinsurancenews.com/colorado-mental-health-insurance-bill-to-help-consumers-navigate-the-system/&quot;,\r\n\t\t\t\t &quot;http://newyork.cbslocal.com/2018/04/04/youtube-headquarters-shooting-suspect/&quot;,\r\n\t\t\t\t &quot;https://ledgergazette.com/2018/04/06/liberty-interactive-co-series-a-liberty-ventures-lvnta-shares-bought-by-brandywine-global-investment-management-llc.html&quot;,\r\n\t\t\t\t &quot;http://bangaloreweekly.com/2018-04-06-city-holding-co-invests-in-cvs-health-corporation-cvs-shares/&quot;,\r\n\t\t\t\t &quot;https://www.thenewsguru.com/didnt-know-lawyer-paid-prostitute-130000-donald-trump/&quot;,\r\n\t\t\t\t &quot;http://www.westlondonsport.com/chelsea/football-wls-conte-gives-two-main-reasons-chelseas-loss-tottenham&quot;,\r\n\t\t\t\t &quot;https://registrarjournal.com/2018/04/06/amazon-com-inc-amzn-shares-bought-by-lenox-wealth-management-inc.html&quot;,\r\n\t\t\t\t &quot;http://www.businessdayonline.com/1bn-eca-withdrawal-commence-action-president-buhari-pdp-tasks-nass/&quot;,\r\n\t\t\t\t &quot;http://www.thesportsbank.net/football/manchester-united/pep-guardiola-asks-for-his-fans-help-vs-united-in-manchester-derby/&quot;,\r\n\t\t\t\t &quot;https://www.pakistantoday.com.pk/2018/04/06/three-palestinians-martyred-as-new-clashes-erupt-along-gaza-border/&quot;,\r\n\t\t\t\t &quot;http://www.nasdaqfortune.com/2018/04/06/risky-factor-of-cvs-health-corporation-cvs-is-observed-at-1-03/&quot;,\r\n\t\t\t\t &quot;https://stocknewstimes.com/2018/04/06/cetera-advisor-networks-llc-decreases-position-in-cvs-health-cvs.html&quot;,\r\n\t\t\t\t &quot;http://nasdaqjournal.com/index.php/2018/04/06/planet-fitness-inc-nyseplnt-do-analysts-think-you-should-buy/&quot;,\r\n\t\t\t\t &quot;http://www.tv360nigeria.com/apc-to-hold-national-congress/&quot;,\r\n\t\t\t\t &quot;https://www.pmnewsnigeria.com/2018/04/03/apc-governors-keep-sealed-lips-after-meeting-with-buhari/&quot;,\r\n\t\t\t\t &quot;https://www.healththoroughfare.com/diet/healthy-lifestyle-best-foods-you-should-eat-for-weight-loss/7061&quot;,\r\n\t\t\t\t &quot;https://stocknewstimes.com/2018/04/05/amazon-com-inc-amzn-shares-bought-by-west-oak-capital-llc.html&quot;,\r\n\t\t\t\t &quot;http://www.current-movie-reviews.com/48428/dr-oz-could-you-be-a-victim-of-sexual-assault-while-on-vacation/&quot;,\r\n\t\t\t\t &quot;https://www.brecorder.com/2018/04/07/410124/world-health-day-to-be-observed-on-april-7/&quot;,\r\n\t\t\t\t &quot;http://www.coloradoindependent.com/169637/trump-pruitt-emissions-epa-pollution&quot;,\r\n\t\t\t\t &quot;https://thecrimereport.org/2018/04/05/will-sessions-new-justice-strategy-turn-the-clock-back-on-civil-rights/&quot;,\r\n\t\t\t\t &quot;http://en.brinkwire.com/245490/pasta-unlikely-to-cause-weight-gain-as-part-of-a-healthy-diet/&quot;]\r\n\t \r\n\t\tp = Pool(15)  # thread count\r\n\t\tp.map(scraper, list1)  # (function, iterable)\r\n\t\tp.terminate()\r\n\t\tp.join()\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/Ie4Q9.png",
            "link": "https://stackoverflow.com/questions/49791271/python-cant-get-counter-to-work-in-multiprocessing-environment-pool-map",
            "title": "Python - Can&#39;t Get Counter To Work in Multiprocessing Environment (Pool, Map)",
            "body": "<p>I need the counter variable (list_counter) inside my 'scraper' function to increment for each iteration through list1. </p>\n\n<p>The problem is it's assigning a counter <strong>to each individual process</strong>. </p>\n\n<p>I want each process to simply increment the global list_counter at the end of the loop, not for each process to have its own counter.</p>\n\n<p>I tried passing the variable as an argument but couldn't get it to work that way either.</p>\n\n<p>What you guys think? Is it even possible to have a global counter work with multiple processes - specifically using pool, map, lock?</p>\n\n<pre><code>from multiprocessing import Lock, Pool\nfrom time import sleep\nfrom bs4 import BeautifulSoup\nimport re\nimport requests\n\nexceptions = []\nlock = Lock()\nlist_counter = 0\n\n\ndef scraper(url):  # url is tied to the individual list items\n    \"\"\"\n    Testing multiprocessing and requests\n    \"\"\"\n\n    global list_counter\n\n    lock.acquire()\n\n    try:\n\n        scrape = requests.get(url,\n                              headers={\"user-agent\": \"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36\"},\n                              timeout=10)\n\n        if scrape.status_code == 200:\n\n            \"\"\" --------------------------------------------- \"\"\"\n            # ---------------------------------------------------\n            '''           --&gt; SCRAPE ALEXA RANK: &lt;--          '''\n            # ---------------------------------------------------\n            \"\"\" --------------------------------------------- \"\"\"\n\n            sleep(0.1)\n            scrape = requests.get(\"http://data.alexa.com/data?cli=10&amp;dat=s&amp;url=\" + url,\n                                  headers={\"user-agent\": \"Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36\"})\n            html = scrape.content\n            soup = BeautifulSoup(html, 'lxml')\n\n            rank = re.findall(r'&lt;popularity[^&gt;]*text=\"(\\d+)\"', str(soup))\n\n            print(\"Server Status:\", scrape.status_code, '-', u\"\\u2713\", '-', list_counter, '-', url, '-', \"Rank:\", rank[0])\n\n            list_counter = list_counter + 1\n\n        else:\n            print(\"Server Status:\", scrape.status_code)\n            list_counter = list_counter + 1\n            print(list_counter)\n            pass\n\n    except BaseException as e:\n        exceptions.append(e)\n        print()\n        print(e)\n        print()\n        list_counter = list_counter + 1\n        print(list_counter)\n        pass\n\n    finally:\n        lock.release()\n\nif __name__ == '__main__':\n\n    list1 = [\"http://www.wallstreetinvestorplace.com/2018/04/cvs-health-corporation-cvs-to-touch-7-54-earnings-growth-for-next-year/\",\n             \"https://macondaily.com/2018/04/06/cetera-advisors-llc-lowers-position-in-cvs-health-cvs.html\",\n             \"http://www.thesportsbank.net/football/liverpool/jurgen-klopp-very-positive-about-mo-salah-injury/\",\n             \"https://www.moneyjournals.com/trump-wasting-time-trying-bring-amazon/\",\n             \"https://www.pmnewsnigeria.com/2018/04/06/fcta-targets-800000-children-for-polio-immunisation/\",\n             \"http://toronto.citynews.ca/2018/04/06/officials-in-canada-braced-for-another-spike-in-illegal-border-crossings/\",\n             \"https://www.pmnewsnigeria.com/2018/04/04/pdp-describes-looters-list-as-plot-to-divert-attention/\",\n             \"https://beyondpesticides.org/dailynewsblog/2018/04/epa-administrator-pruitt-colluding-regulated-industry/\",\n             \"http://thyblackman.com/2018/04/06/robert-mueller-is-searching-for/\",\n             \"https://www.theroar.com.au/2018/04/06/2018-commonwealth-games-swimming-night-2-finals-live-updates-results-blog/\",\n             \"https://medicalresearch.com/pain-research/migraine-linked-to-increased-risk-of-heart-disease-and-stroke/40858/\",\n             \"http://www.investingbizz.com/2018/04/amazon-com-inc-amzn-stock-creates-investors-concerns/\",\n             \"https://stocknewstimes.com/2018/04/06/convergence-investment-partners-llc-grows-position-in-amazon-com-inc-amzn.html\",\n             \"https://factsherald.com/old-food-rules-needs-to-be-updated/\",\n             \"https://www.nextadvisor.com/blog/2018/04/06/the-facebook-scandal-evolves/\",\n             \"http://sacramento.cbslocal.com/2018/04/04/police-family-youtube-shooter/\",\n             \"http://en.brinkwire.com/245768/why-does-stress-lead-to-weight-gain-study-sheds-light/\",\n             \"https://www.marijuana.com/news/2018/04/monterey-bud-jeff-sessions-is-on-the-wrong-side-of-history-science-and-public-opinion/\",\n             \"http://www.stocksgallery.com/2018/04/06/jpmorgan-chase-co-jpm-noted-a-price-change-of-0-80-and-amazon-com-inc-amzn-closes-with-a-move-of-2-92/\",\n             \"https://stocknewstimes.com/2018/04/06/front-barnett-associates-llc-has-2-41-million-position-in-cvs-health-corp-cvs.html\",\n             \"http://www.liveinsurancenews.com/colorado-mental-health-insurance-bill-to-help-consumers-navigate-the-system/\",\n             \"http://newyork.cbslocal.com/2018/04/04/youtube-headquarters-shooting-suspect/\",\n             \"https://ledgergazette.com/2018/04/06/liberty-interactive-co-series-a-liberty-ventures-lvnta-shares-bought-by-brandywine-global-investment-management-llc.html\",\n             \"http://bangaloreweekly.com/2018-04-06-city-holding-co-invests-in-cvs-health-corporation-cvs-shares/\",\n             \"https://www.thenewsguru.com/didnt-know-lawyer-paid-prostitute-130000-donald-trump/\",\n             \"http://www.westlondonsport.com/chelsea/football-wls-conte-gives-two-main-reasons-chelseas-loss-tottenham\",\n             \"https://registrarjournal.com/2018/04/06/amazon-com-inc-amzn-shares-bought-by-lenox-wealth-management-inc.html\",\n             \"http://www.businessdayonline.com/1bn-eca-withdrawal-commence-action-president-buhari-pdp-tasks-nass/\",\n             \"http://www.thesportsbank.net/football/manchester-united/pep-guardiola-asks-for-his-fans-help-vs-united-in-manchester-derby/\",\n             \"https://www.pakistantoday.com.pk/2018/04/06/three-palestinians-martyred-as-new-clashes-erupt-along-gaza-border/\",\n             \"http://www.nasdaqfortune.com/2018/04/06/risky-factor-of-cvs-health-corporation-cvs-is-observed-at-1-03/\",\n             \"https://stocknewstimes.com/2018/04/06/cetera-advisor-networks-llc-decreases-position-in-cvs-health-cvs.html\",\n             \"http://nasdaqjournal.com/index.php/2018/04/06/planet-fitness-inc-nyseplnt-do-analysts-think-you-should-buy/\",\n             \"http://www.tv360nigeria.com/apc-to-hold-national-congress/\",\n             \"https://www.pmnewsnigeria.com/2018/04/03/apc-governors-keep-sealed-lips-after-meeting-with-buhari/\",\n             \"https://www.healththoroughfare.com/diet/healthy-lifestyle-best-foods-you-should-eat-for-weight-loss/7061\",\n             \"https://stocknewstimes.com/2018/04/05/amazon-com-inc-amzn-shares-bought-by-west-oak-capital-llc.html\",\n             \"http://www.current-movie-reviews.com/48428/dr-oz-could-you-be-a-victim-of-sexual-assault-while-on-vacation/\",\n             \"https://www.brecorder.com/2018/04/07/410124/world-health-day-to-be-observed-on-april-7/\",\n             \"http://www.coloradoindependent.com/169637/trump-pruitt-emissions-epa-pollution\",\n             \"https://thecrimereport.org/2018/04/05/will-sessions-new-justice-strategy-turn-the-clock-back-on-civil-rights/\",\n             \"http://en.brinkwire.com/245490/pasta-unlikely-to-cause-weight-gain-as-part-of-a-healthy-diet/\"]\n\n    p = Pool(15)  # thread count\n    p.map(scraper, list1)  # (function, iterable)\n    p.terminate()\n    p.join()\n</code></pre>\n"
        },
        {
            "tags": [
                "spring",
                "exception-handling",
                "spring-ws"
            ],
            "owner": {
                "reputation": 129,
                "user_id": 1863111,
                "user_type": "registered",
                "accept_rate": 20,
                "profile_image": "https://www.gravatar.com/avatar/92163ffd04b48357c3833dfe6d2096ad?s=128&d=identicon&r=PG",
                "display_name": "lukisp",
                "link": "https://stackoverflow.com/users/1863111/lukisp"
            },
            "is_answered": false,
            "view_count": 4,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1523524153,
            "creation_date": 1523524153,
            "question_id": 49792474,
            "body_markdown": "I have soap andpoint which should return response type A based on request type B. But during processing of request, i&#39;m expecting errors (like unable to call downastream service) which throw cutom exception for example type ExpEx. And now i wat to do custom error mapping, because in case of errors I don&#39;t want to return type A but want to return type CFault (which is defined in wsd also).\r\nNow question: \r\n- is is possible to create custom eero handle which rturn CFault instead A\r\n- or is it possible to make enpoint allow to return two types of response A and CFault (I think Object) ? \r\n\r\nmy enpoint:\r\n\r\n    public class FantasticEndpoint extend WebServiceEndpoint {\r\n\tprivate static final String NAMESPACE = &quot;http://www.fantastic.com/SOA/tmp/FantasticService/v_2_4&quot;;\r\n    \t\r\n\t@PayloadRoot(namespace = NAMESPACE, localPart = &quot;handleBOperation&quot;)\r\n\t@ResponsePayload\r\n\tpublic A createConsumers(@RequestPayload B b{\r\n\t\t//do some dangerous logic possility throw EXCEPTION\r\n        \t// if EXCEPTION return CFault or return A if normal processing\r\n\t}\r\n}",
            "link": "https://stackoverflow.com/questions/49792474/spring-ws-return-valid-response-on-exception",
            "title": "Spring-WS - return valid response on Exception",
            "body": "<p>I have soap andpoint which should return response type A based on request type B. But during processing of request, i'm expecting errors (like unable to call downastream service) which throw cutom exception for example type ExpEx. And now i wat to do custom error mapping, because in case of errors I don't want to return type A but want to return type CFault (which is defined in wsd also).\nNow question: \n- is is possible to create custom eero handle which rturn CFault instead A\n- or is it possible to make enpoint allow to return two types of response A and CFault (I think Object) ? </p>\n\n<p>my enpoint:</p>\n\n<pre><code>public class FantasticEndpoint extend WebServiceEndpoint {\nprivate static final String NAMESPACE = \"http://www.fantastic.com/SOA/tmp/FantasticService/v_2_4\";\n\n@PayloadRoot(namespace = NAMESPACE, localPart = \"handleBOperation\")\n@ResponsePayload\npublic A createConsumers(@RequestPayload B b{\n    //do some dangerous logic possility throw EXCEPTION\n        // if EXCEPTION return CFault or return A if normal processing\n}\n</code></pre>\n\n<p>}</p>\n"
        },
        {
            "tags": [
                "python-3.x",
                "email"
            ],
            "owner": {
                "reputation": 62,
                "user_id": 6315135,
                "user_type": "registered",
                "accept_rate": 73,
                "profile_image": "https://www.gravatar.com/avatar/ec6ebc80a755f2f69daec24126238184?s=128&d=identicon&r=PG&f=1",
                "display_name": "mathmaniage",
                "link": "https://stackoverflow.com/users/6315135/mathmaniage"
            },
            "is_answered": false,
            "view_count": 8,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1523524149,
            "creation_date": 1523524149,
            "question_id": 49792473,
            "body_markdown": "I know there are a lots of how to send mail questions before, but does sending mail from python scripts not work anymore? I&#39;ve tried to use the SMTP lib with the following script:   \r\n\r\n      \r\n    import smtplib\r\n    fromaddr = &#39;mymail@gmail.com&#39;\r\n    toaddr = &#39;someones@gmail.com&#39;\r\n    msg = &quot;\\r\\n&quot;.join([\r\n        &quot;From: mymail@gmail.com&quot;,\r\n        &quot;To: someonesmail@gmail.com&quot;,\r\n        &quot;Subject: Just a message&quot;,\r\n        &quot;&quot;,    \r\n        &#39;Why, oh why?&#39;\r\n        ])\r\n       \r\n    username = &#39;mymail@gmail.com&#39;\r\n    pwd = &#39;mypassword&#39;\r\n    server = smtplib.SMTP(&#39;smtp.gmail.com:587&#39;)\r\n    server.ehlo()\r\n    server.starttls()\r\n    server.login(username, pwd)\r\n    server.sendmail(fromaddr, toaddrs, msg)\r\n    server.quit()  \r\n  \r\nBut it says to login via a web browser, and [this post][1] says that google now uses an api to send mails. So, having confusion about the possibility to send mails via scripts without using any of google&#39;s api, Is it possible?\r\n\r\n\r\n  [1]: https://stackoverflow.com/questions/10147455/how-to-send-an-email-with-gmail-as-provider-using-python",
            "link": "https://stackoverflow.com/questions/49792473/sending-mail-from-python-in-2018",
            "title": "sending mail from python in 2018",
            "body": "<p>I know there are a lots of how to send mail questions before, but does sending mail from python scripts not work anymore? I've tried to use the SMTP lib with the following script:   </p>\n\n<pre><code>import smtplib\nfromaddr = 'mymail@gmail.com'\ntoaddr = 'someones@gmail.com'\nmsg = \"\\r\\n\".join([\n    \"From: mymail@gmail.com\",\n    \"To: someonesmail@gmail.com\",\n    \"Subject: Just a message\",\n    \"\",    \n    'Why, oh why?'\n    ])\n\nusername = 'mymail@gmail.com'\npwd = 'mypassword'\nserver = smtplib.SMTP('smtp.gmail.com:587')\nserver.ehlo()\nserver.starttls()\nserver.login(username, pwd)\nserver.sendmail(fromaddr, toaddrs, msg)\nserver.quit()  \n</code></pre>\n\n<p>But it says to login via a web browser, and <a href=\"https://stackoverflow.com/questions/10147455/how-to-send-an-email-with-gmail-as-provider-using-python\">this post</a> says that google now uses an api to send mails. So, having confusion about the possibility to send mails via scripts without using any of google's api, Is it possible?</p>\n"
        },
        {
            "tags": [
                "proxy",
                "routing"
            ],
            "owner": {
                "reputation": 11,
                "user_id": 2675880,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/59382d677e3c159486e4ef235ffb9093?s=128&d=identicon&r=PG&f=1",
                "display_name": "Dave Gitenburgh",
                "link": "https://stackoverflow.com/users/2675880/dave-gitenburgh"
            },
            "is_answered": false,
            "view_count": 4,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1523524144,
            "creation_date": 1523523818,
            "last_edit_date": 1523524144,
            "question_id": 49792353,
            "body_markdown": "I have a client that cannot utilize proxy servers\r\nThe client is trying to access directly to a certain IP which is not possible, all external traffic is flowing through the proxy\r\n\r\n\r\nfor example:\r\n\r\nDestination IP is: 157.55.253.159\r\n\r\nTarget url: https://msrsplatdemo.cloudapp.net/hello\r\n\r\nproxy: 10.22.1.1\r\n\r\n\r\ncurrent behavior - client is trying to access directly to https://msrsplatdemo.cloudapp.net/hello but is not even successful to resolve it.\r\nadded it to the hosts file so resolution is not an issue anymore\r\nnow its trying to access 157.55.253.159 directly but this IP does not exists on the network.\r\n\r\nHow can i route the traffic to pass through the proxy or force the client to utilize the proxy somehow?\r\n\r\nThanks",
            "link": "https://stackoverflow.com/questions/49792353/redirect-direct-https-traffic-to-a-specific-address-ip-to-a-proxy-through-a-pr",
            "title": "Redirect direct https traffic to a specific address/IP to a proxy &amp; through a proxy",
            "body": "<p>I have a client that cannot utilize proxy servers\nThe client is trying to access directly to a certain IP which is not possible, all external traffic is flowing through the proxy</p>\n\n<p>for example:</p>\n\n<p>Destination IP is: 157.55.253.159</p>\n\n<p>Target url: <a href=\"https://msrsplatdemo.cloudapp.net/hello\" rel=\"nofollow noreferrer\">https://msrsplatdemo.cloudapp.net/hello</a></p>\n\n<p>proxy: 10.22.1.1</p>\n\n<p>current behavior - client is trying to access directly to <a href=\"https://msrsplatdemo.cloudapp.net/hello\" rel=\"nofollow noreferrer\">https://msrsplatdemo.cloudapp.net/hello</a> but is not even successful to resolve it.\nadded it to the hosts file so resolution is not an issue anymore\nnow its trying to access 157.55.253.159 directly but this IP does not exists on the network.</p>\n\n<p>How can i route the traffic to pass through the proxy or force the client to utilize the proxy somehow?</p>\n\n<p>Thanks</p>\n"
        },
        {
            "tags": [
                "javascript",
                "python",
                "numpy"
            ],
            "owner": {
                "reputation": 137,
                "user_id": 2723463,
                "user_type": "registered",
                "accept_rate": 52,
                "profile_image": "https://i.stack.imgur.com/WHRVb.png?s=128&g=1",
                "display_name": "WebSight",
                "link": "https://stackoverflow.com/users/2723463/websight"
            },
            "is_answered": true,
            "view_count": 25,
            "accepted_answer_id": 49792472,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1523524141,
            "creation_date": 1523522295,
            "question_id": 49791780,
            "body_markdown": "I have this piece of code that i am trying to convert from Python to Javascript. It is a non-maximum suppression function. It seeks to remove maximum values around a local maximum in a 2D array of floating point values. Since it is using a numpy function, it does not convert easily to javascript.\r\n\r\n     def non_max_suppression(plain, window_size=3, threshold=NMS_Threshold):\r\n            under_threshold_indices = plain &lt; threshold\r\n            plain[under_threshold_indices] = 0\r\n            return plain * (plain == maximum_filter(plain, footprint=np.ones((window_size, window_size))))\r\n\r\nHere is an approximate conversion, still in Python. However, the result is not the same. It provides approximately the same result but some of the maximums are suppressed that should be kept.\r\n   \r\n \r\n\r\n    def non_max_suppression(plain, window_size=3, threshold=NMS_Threshold):\r\n            under_threshold_indices = plain &lt; threshold\r\n            plain[under_threshold_indices] = 0\r\n            outter = np.zeros((46, 54))\r\n            for i in range(window_size, 41, window_size):\r\n                for j in range(window_size, 49, window_size):\r\n                    maxVal = 0;\r\n                    maxLoc = [-1,-1]\r\n                    for k in range(i-window_size, i+window_size):\r\n                        for h in range(j-window_size, j+window_size):\r\n                            if(plain[k][h]&gt;maxVal):\r\n                                maxVal = plain[k][h]\r\n                                maxLoc = [k,h]\r\n                    if(maxVal&gt;0):                            \r\n                        outter[maxLoc[0]][maxLoc[1]]=maxVal\r\n            return outter\r\n\r\nThis is easy to convert to Javascript.\r\n\r\n\r\n    function makeArray(w, h, val) {\r\n        var arr = [];\r\n        for(i = 0; i &lt; w; i++) {\r\n            arr[i] = [];\r\n            for(j = 0; j &lt; h; j++) {\r\n                arr[i][j] = val;\r\n            }\r\n        }\r\n        return arr;\r\n    }\r\n\r\n    non_max_suppression(plain, window_size=3, threshold=this.NMS_Threshold) {\r\n            var nmsImage = makeArray(plain.length, plain[0].length,0);\r\n            for(var i=0; i&lt;(plain.length); i+=1){\r\n                for(var j=0; j&lt;(plain[i].length); j+=1){\r\n                    if(plain[i][j]&lt;threshold) {\r\n                        plain[i][j]=0;\r\n                    }\r\n                }\r\n            }\r\n            for(var i=window_size; i&lt;(plain.length-window_size); i+=window_size){\r\n                for(var j=window_size; j&lt;(plain[i].length-window_size); j+=window_size){\r\n                    var maxVal = 0;\r\n                    var maxLoc = [-1,-1];\r\n                    for(var k=i-3; k&lt;i+3; k+=1){\r\n                        for(var h=j-3; h&lt;j+3; h+=1){\r\n                            if((plain[k][h]&gt;maxVal) &amp;&amp; (plain[k][h]&gt;=threshold)){ \r\n                                maxVal = plain[k][h];\r\n                                maxLoc = [k,h];\r\n                            }\r\n                            //plain[k][h]=0;\r\n                        }\r\n                    }\r\n                    if(maxVal&gt;0) {\r\n                        nmsImage[maxLoc[0]][maxLoc[1]] = maxVal;\r\n                    }\r\n                }\r\n            }\r\n            return nmsImage;\r\n        }\r\n\r\nAny advice on how to correct this?",
            "link": "https://stackoverflow.com/questions/49791780/python-to-javascript-numpy-issue",
            "title": "Python To Javascript - Numpy Issue",
            "body": "<p>I have this piece of code that i am trying to convert from Python to Javascript. It is a non-maximum suppression function. It seeks to remove maximum values around a local maximum in a 2D array of floating point values. Since it is using a numpy function, it does not convert easily to javascript.</p>\n\n<pre><code> def non_max_suppression(plain, window_size=3, threshold=NMS_Threshold):\n        under_threshold_indices = plain &lt; threshold\n        plain[under_threshold_indices] = 0\n        return plain * (plain == maximum_filter(plain, footprint=np.ones((window_size, window_size))))\n</code></pre>\n\n<p>Here is an approximate conversion, still in Python. However, the result is not the same. It provides approximately the same result but some of the maximums are suppressed that should be kept.</p>\n\n<pre><code>def non_max_suppression(plain, window_size=3, threshold=NMS_Threshold):\n        under_threshold_indices = plain &lt; threshold\n        plain[under_threshold_indices] = 0\n        outter = np.zeros((46, 54))\n        for i in range(window_size, 41, window_size):\n            for j in range(window_size, 49, window_size):\n                maxVal = 0;\n                maxLoc = [-1,-1]\n                for k in range(i-window_size, i+window_size):\n                    for h in range(j-window_size, j+window_size):\n                        if(plain[k][h]&gt;maxVal):\n                            maxVal = plain[k][h]\n                            maxLoc = [k,h]\n                if(maxVal&gt;0):                            \n                    outter[maxLoc[0]][maxLoc[1]]=maxVal\n        return outter\n</code></pre>\n\n<p>This is easy to convert to Javascript.</p>\n\n<pre><code>function makeArray(w, h, val) {\n    var arr = [];\n    for(i = 0; i &lt; w; i++) {\n        arr[i] = [];\n        for(j = 0; j &lt; h; j++) {\n            arr[i][j] = val;\n        }\n    }\n    return arr;\n}\n\nnon_max_suppression(plain, window_size=3, threshold=this.NMS_Threshold) {\n        var nmsImage = makeArray(plain.length, plain[0].length,0);\n        for(var i=0; i&lt;(plain.length); i+=1){\n            for(var j=0; j&lt;(plain[i].length); j+=1){\n                if(plain[i][j]&lt;threshold) {\n                    plain[i][j]=0;\n                }\n            }\n        }\n        for(var i=window_size; i&lt;(plain.length-window_size); i+=window_size){\n            for(var j=window_size; j&lt;(plain[i].length-window_size); j+=window_size){\n                var maxVal = 0;\n                var maxLoc = [-1,-1];\n                for(var k=i-3; k&lt;i+3; k+=1){\n                    for(var h=j-3; h&lt;j+3; h+=1){\n                        if((plain[k][h]&gt;maxVal) &amp;&amp; (plain[k][h]&gt;=threshold)){ \n                            maxVal = plain[k][h];\n                            maxLoc = [k,h];\n                        }\n                        //plain[k][h]=0;\n                    }\n                }\n                if(maxVal&gt;0) {\n                    nmsImage[maxLoc[0]][maxLoc[1]] = maxVal;\n                }\n            }\n        }\n        return nmsImage;\n    }\n</code></pre>\n\n<p>Any advice on how to correct this?</p>\n"
        },
        {
            "tags": [
                "node.js",
                "momentjs"
            ],
            "owner": {
                "reputation": 32,
                "user_id": 7344682,
                "user_type": "registered",
                "accept_rate": 33,
                "profile_image": "https://www.gravatar.com/avatar/12f3ad237b9a8dd3d25702e1cd6ce111?s=128&d=identicon&r=PG&f=1",
                "display_name": "Shaju Nr",
                "link": "https://stackoverflow.com/users/7344682/shaju-nr"
            },
            "is_answered": false,
            "view_count": 16,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1523524134,
            "creation_date": 1523524134,
            "question_id": 49792470,
            "body_markdown": "I converted a time_stamp in postgres using moment in node js.From the console the timestamp is shown as 2018-01-31T18:30:00.000Z .When converting this date with moment gives the output 2018-02-01 00:00\r\nFollowing is the code\r\n\r\n    console.log(moment(invoice.bill_period_from).format(&#39;YYYY-MM-DD HH:mm&#39;));\r\n\r\nHow to convert the date exactly as in the database in node js .",
            "link": "https://stackoverflow.com/questions/49792470/moment-in-node-js-output-varies-on-time-with-z",
            "title": "Moment in node js output varies on time with Z",
            "body": "<p>I converted a time_stamp in postgres using moment in node js.From the console the timestamp is shown as 2018-01-31T18:30:00.000Z .When converting this date with moment gives the output 2018-02-01 00:00\nFollowing is the code</p>\n\n<pre><code>console.log(moment(invoice.bill_period_from).format('YYYY-MM-DD HH:mm'));\n</code></pre>\n\n<p>How to convert the date exactly as in the database in node js .</p>\n"
        },
        {
            "tags": [
                "java",
                "spring-boot",
                "oauth-2.0",
                "spring-security-oauth2"
            ],
            "owner": {
                "reputation": 59,
                "user_id": 762594,
                "user_type": "registered",
                "accept_rate": 88,
                "profile_image": "https://www.gravatar.com/avatar/3736bbbddceaa0b3ab26c72cd8bfc0aa?s=128&d=identicon&r=PG",
                "display_name": "Qbix",
                "link": "https://stackoverflow.com/users/762594/qbix"
            },
            "is_answered": false,
            "view_count": 18,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1523524133,
            "creation_date": 1523174788,
            "last_edit_date": 1523524133,
            "question_id": 49715769,
            "body_markdown": "I&#39;m trying to use authentication by google. I am using springboot2, so most of the configuration is automatic. The authentication itself works good, but afterwards I would like to populate Principal with my own data (roles, username, and stuff).\r\n\r\nI&#39;ve created MyUserService that exteds DefaultOauth2UserService, and I am trying to use it as follows:\r\n\r\n    @Configuration\r\n    public class SecurityConfig extends WebSecurityConfigurerAdapter {\r\n        @Autowired\r\n        MyUserService myUserService;\r\n\r\n        @Override\r\n        protected void configure(HttpSecurity http) throws Exception {\r\n            http\r\n                .authorizeRequests()\r\n                    .anyRequest().authenticated()\r\n                    .and()\r\n                .oauth2Login()\r\n                    .userInfoEndpoint()\r\n                        .userService(myUserService);\r\n        }\r\n    }\r\n\r\nI&#39;ve checked with debuger, that application never actually uses loadUser methods. And here is implementation of MyUserService:\r\n\r\n    @Component\r\n    public class MyUserService extends DefaultOAuth2UserService {\r\n        @Autowired\r\n        UserRepository userRepository;\r\n\r\n        public MyUserService(){\r\n            LoggerFactory.getLogger(MyUserService.class).info(&quot;initializing user service&quot;);\r\n        }\r\n\r\n        @Override\r\n        public OAuth2User loadUser(OAuth2UserRequest userRequest) throws OAuth2AuthenticationException {\r\n            OAuth2User oAuth2User = super.loadUser(userRequest);\r\n            Map&lt;String, Object&gt; attributes = oAuth2User.getAttributes();\r\n\r\n            String emailFromGoogle = (String) attributes.get(&quot;email&quot;);\r\n            User user = userRepository.findByEmail(emailFromGoogle);\r\n            attributes.put(&quot;given_name&quot;, user.getFirstName());\r\n            attributes.put(&quot;family_name&quot;, user.getLastName());\r\n\r\n            Set&lt;GrantedAuthority&gt; authoritySet = new HashSet&lt;&gt;(oAuth2User.getAuthorities());\r\n\r\n            return new DefaultOAuth2User(authoritySet, attributes, &quot;sub&quot;);\r\n        }\r\n    }",
            "link": "https://stackoverflow.com/questions/49715769/why-is-my-oauth2-config-not-using-my-custom-userservice",
            "title": "Why is my oauth2 config not using my custom UserService?",
            "body": "<p>I'm trying to use authentication by google. I am using springboot2, so most of the configuration is automatic. The authentication itself works good, but afterwards I would like to populate Principal with my own data (roles, username, and stuff).</p>\n\n<p>I've created MyUserService that exteds DefaultOauth2UserService, and I am trying to use it as follows:</p>\n\n<pre><code>@Configuration\npublic class SecurityConfig extends WebSecurityConfigurerAdapter {\n    @Autowired\n    MyUserService myUserService;\n\n    @Override\n    protected void configure(HttpSecurity http) throws Exception {\n        http\n            .authorizeRequests()\n                .anyRequest().authenticated()\n                .and()\n            .oauth2Login()\n                .userInfoEndpoint()\n                    .userService(myUserService);\n    }\n}\n</code></pre>\n\n<p>I've checked with debuger, that application never actually uses loadUser methods. And here is implementation of MyUserService:</p>\n\n<pre><code>@Component\npublic class MyUserService extends DefaultOAuth2UserService {\n    @Autowired\n    UserRepository userRepository;\n\n    public MyUserService(){\n        LoggerFactory.getLogger(MyUserService.class).info(\"initializing user service\");\n    }\n\n    @Override\n    public OAuth2User loadUser(OAuth2UserRequest userRequest) throws OAuth2AuthenticationException {\n        OAuth2User oAuth2User = super.loadUser(userRequest);\n        Map&lt;String, Object&gt; attributes = oAuth2User.getAttributes();\n\n        String emailFromGoogle = (String) attributes.get(\"email\");\n        User user = userRepository.findByEmail(emailFromGoogle);\n        attributes.put(\"given_name\", user.getFirstName());\n        attributes.put(\"family_name\", user.getLastName());\n\n        Set&lt;GrantedAuthority&gt; authoritySet = new HashSet&lt;&gt;(oAuth2User.getAuthorities());\n\n        return new DefaultOAuth2User(authoritySet, attributes, \"sub\");\n    }\n}\n</code></pre>\n"
        },
        {
            "tags": [
                "azure",
                "azure-webjobs"
            ],
            "owner": {
                "reputation": 2260,
                "user_id": 1033684,
                "user_type": "registered",
                "accept_rate": 67,
                "profile_image": "https://i.stack.imgur.com/sKoNw.jpg?s=128&g=1",
                "display_name": "Rob Sedgwick",
                "link": "https://stackoverflow.com/users/1033684/rob-sedgwick"
            },
            "is_answered": false,
            "view_count": 9,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1523524132,
            "creation_date": 1523524132,
            "question_id": 49792469,
            "body_markdown": "I want to run a web job if a specific email is received. Currently, I use a TimerTrigger and create an ImapClient which reads the unread emails and scans for one with the appropriate content. This works fine but there is a delay of whatever duration I set my timer to. Ideally, I would like an IMapTrigger (if such a thing exists) which would fire whenever a new email appears.\r\n\r\nDoes a) such a thing exist, b) is it worth writing one?",
            "link": "https://stackoverflow.com/questions/49792469/imaptrigger-for-azure-web-jobs",
            "title": "IMapTrigger for Azure web jobs",
            "body": "<p>I want to run a web job if a specific email is received. Currently, I use a TimerTrigger and create an ImapClient which reads the unread emails and scans for one with the appropriate content. This works fine but there is a delay of whatever duration I set my timer to. Ideally, I would like an IMapTrigger (if such a thing exists) which would fire whenever a new email appears.</p>\n\n<p>Does a) such a thing exist, b) is it worth writing one?</p>\n"
        },
        {
            "tags": [
                "python",
                "string",
                "variables",
                "exec"
            ],
            "owner": {
                "reputation": 73,
                "user_id": 6746639,
                "user_type": "registered",
                "accept_rate": 48,
                "profile_image": "https://i.stack.imgur.com/J4msH.jpg?s=128&g=1",
                "display_name": "delinco",
                "link": "https://stackoverflow.com/users/6746639/delinco"
            },
            "is_answered": false,
            "view_count": 21,
            "closed_date": 1523524895,
            "answer_count": 0,
            "score": 1,
            "last_activity_date": 1523524129,
            "creation_date": 1523522676,
            "question_id": 49791920,
            "body_markdown": "    import networkx as nx\r\n    import string\r\n    import random\r\n   \r\n\r\n    females = list(string.ascii_lowercase)\r\n    males = list(string.ascii_uppercase)\r\n    \r\n    class User_():\r\n        def __init__(self, id_):\r\n            self.id_ = id_\r\n            self.G = nx.DiGraph()\r\n            seed = random.randint(0,26)\r\n            \r\n            frnd_User_ = []\r\n            for i in range(0, seed):\r\n                frnd_User_.append(males[random.randint(0,25)])\r\n                frnd_User_.append(females[random.randint(0,25)])\r\n            self.frnd_User_ = frnd_User_\r\n            \r\n            edge_User_ = []\r\n            for i in range(0, len(frnd_User_)):\r\n                edge_User_.append((self.id_,frnd_User_[i]))\r\n            self.edge_User_ = edge_User_\r\n            \r\n            self.G.add_edges_from(edge_User_)\r\n\r\n\r\nFor this given code defining User_(), I&#39;d like to generate User_ class for each letters in the list males. For example, I&#39;d like to do following task using for loop:\r\n\r\nA = User_(&#39;A&#39;)\r\n\r\nB = User_(&#39;B&#39;)\r\n\r\n...\r\n\r\nZ = User_(&#39;Z&#39;)\r\n\r\nI&#39;d tried follwoing codes but get returned with the error : \r\n\r\n    for i in range(0, 25):\r\n        males[i] = exec(&quot;%s = %d&quot; % (males[i],User_(males[i])))\r\n\r\n    TypeError                                 Traceback (most recent call last)\r\n    &lt;ipython-input-24-d6ab5c2813a4&gt; in &lt;module&gt;()\r\n          1 for i in range(0, 25):\r\n    ----&gt; 2     males[i] = exec(&quot;%s = %d&quot; % (males[i],User_(males[i])))\r\n    \r\n    TypeError: %d format: a number is required, not User_\r\n\r\nAny advice to proceed?\r\n\r\n",
            "link": "https://stackoverflow.com/questions/49791920/convert-string-in-list-into-class-stored-variable-in-python",
            "closed_reason": "duplicate",
            "title": "Convert String in List into Class Stored Variable in Python",
            "body": "<pre><code>import networkx as nx\nimport string\nimport random\n\n\nfemales = list(string.ascii_lowercase)\nmales = list(string.ascii_uppercase)\n\nclass User_():\n    def __init__(self, id_):\n        self.id_ = id_\n        self.G = nx.DiGraph()\n        seed = random.randint(0,26)\n\n        frnd_User_ = []\n        for i in range(0, seed):\n            frnd_User_.append(males[random.randint(0,25)])\n            frnd_User_.append(females[random.randint(0,25)])\n        self.frnd_User_ = frnd_User_\n\n        edge_User_ = []\n        for i in range(0, len(frnd_User_)):\n            edge_User_.append((self.id_,frnd_User_[i]))\n        self.edge_User_ = edge_User_\n\n        self.G.add_edges_from(edge_User_)\n</code></pre>\n\n<p>For this given code defining User_(), I'd like to generate User_ class for each letters in the list males. For example, I'd like to do following task using for loop:</p>\n\n<p>A = User_('A')</p>\n\n<p>B = User_('B')</p>\n\n<p>...</p>\n\n<p>Z = User_('Z')</p>\n\n<p>I'd tried follwoing codes but get returned with the error : </p>\n\n<pre><code>for i in range(0, 25):\n    males[i] = exec(\"%s = %d\" % (males[i],User_(males[i])))\n\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-24-d6ab5c2813a4&gt; in &lt;module&gt;()\n      1 for i in range(0, 25):\n----&gt; 2     males[i] = exec(\"%s = %d\" % (males[i],User_(males[i])))\n\nTypeError: %d format: a number is required, not User_\n</code></pre>\n\n<p>Any advice to proceed?</p>\n"
        },
        {
            "tags": [
                "c#",
                "api",
                "identity"
            ],
            "owner": {
                "reputation": 14,
                "user_id": 7388204,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/e8e3bfbf0ed5a5c72be4d2c0e70e1c30?s=128&d=identicon&r=PG&f=1",
                "display_name": "user7388204",
                "link": "https://stackoverflow.com/users/7388204/user7388204"
            },
            "is_answered": false,
            "view_count": 10,
            "answer_count": 0,
            "score": -1,
            "last_activity_date": 1523524125,
            "creation_date": 1523524125,
            "question_id": 49792461,
            "body_markdown": "I want to be able to change my password by webAPI for a mobile application. As I&#39;m using Identity I need to use the ChangePassword-method from the Account-controller. This method however takes in a User.Identity.GetUserId like this:\r\n\r\n    IdentityResult result = await UserManager.ChangePasswordAsync(User.Identity.GetUserId(), model.OldPassword,\r\n                model.NewPassword);\r\n\r\nWhen you login to the mobile app you get a token, are you supposed to send in this in the request to the changePassword-method? How is GetUserId working here? I&#39;ve been searching but can&#39;t come up with anything. ",
            "link": "https://stackoverflow.com/questions/49792461/change-password-by-webapi-for-mobile-application",
            "title": "Change password by webAPI for mobile application",
            "body": "<p>I want to be able to change my password by webAPI for a mobile application. As I'm using Identity I need to use the ChangePassword-method from the Account-controller. This method however takes in a User.Identity.GetUserId like this:</p>\n\n<pre><code>IdentityResult result = await UserManager.ChangePasswordAsync(User.Identity.GetUserId(), model.OldPassword,\n            model.NewPassword);\n</code></pre>\n\n<p>When you login to the mobile app you get a token, are you supposed to send in this in the request to the changePassword-method? How is GetUserId working here? I've been searching but can't come up with anything. </p>\n"
        },
        {
            "tags": [
                "javascript",
                "reactjs",
                "heroku",
                "production-environment",
                "create-react-app"
            ],
            "owner": {
                "reputation": 485,
                "user_id": 6553898,
                "user_type": "registered",
                "accept_rate": 50,
                "profile_image": "https://lh4.googleusercontent.com/-oM1NMvGT2sA/AAAAAAAAAAI/AAAAAAAABnA/Rti1Z_odx0c/photo.jpg?sz=128",
                "display_name": "Nicolas M. Pardo",
                "link": "https://stackoverflow.com/users/6553898/nicolas-m-pardo"
            },
            "is_answered": false,
            "view_count": 12,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1523524125,
            "creation_date": 1523474185,
            "question_id": 49782903,
            "body_markdown": "I&#39;m working with a create-react-app generated app which I&#39;m deploying to Heroku. I would like to test out the production version `npm run build` in an actual server.\r\n\r\nIs it possible to serve the build version instead of the development version to heroku? It&#39;s okay if I can no longer have the dev version up in that heroku instance.\r\n\r\nIf possible, *how can I do so*, I have been searching up and nothing comes up",
            "link": "https://stackoverflow.com/questions/49782903/deploy-create-react-app-build-version-to-heroku",
            "title": "Deploy create-react-app build version to Heroku",
            "body": "<p>I'm working with a create-react-app generated app which I'm deploying to Heroku. I would like to test out the production version <code>npm run build</code> in an actual server.</p>\n\n<p>Is it possible to serve the build version instead of the development version to heroku? It's okay if I can no longer have the dev version up in that heroku instance.</p>\n\n<p>If possible, <em>how can I do so</em>, I have been searching up and nothing comes up</p>\n"
        },
        {
            "tags": [
                "algorithm",
                "data-structures",
                "quicksort"
            ],
            "owner": {
                "reputation": 6,
                "user_id": 9253439,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/60f2f121c85c7b2b2811a21677b699da?s=128&d=identicon&r=PG&f=1",
                "display_name": "Priyanka",
                "link": "https://stackoverflow.com/users/9253439/priyanka"
            },
            "is_answered": false,
            "view_count": 19,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1523524121,
            "creation_date": 1523521208,
            "last_edit_date": 1523524121,
            "question_id": 49791404,
            "body_markdown": "Implemented this Quick sort Algorithm in javascript\r\n\r\n\r\n    function  Quick_Sort(arr,l,h) {\r\n   \r\n        var temp;\r\n        var low = l;\r\n        var high = h;\r\n        var pivot = arr[Math.floor((low+high)/2)];\r\n  \r\n        while(low&lt;=high) {\r\n             while(arr[low] &lt; pivot) {\r\n                low++;\r\n             }\r\n            while(pivot &lt; arr[high]) {\r\n                high--;\r\n            }\r\n            if(low &lt;= high) {\r\n                temp = arr[low];  \r\n                arr[low] = arr[high];\r\n                arr[high] = temp;\r\n                low++;\r\n                high--;\r\n            }\r\n        }\r\n        return low;\r\n    }\r\n \r\n    function sort(arr,low,high) {\r\n \r\n        var low = low || 0;\r\n        var high = high || arr.length - 1;\r\n        var pivotIndex =  Quick_Sort(arr,low,high);\r\n\r\n        if(low &lt; pivotIndex-1) {   \r\n            sort(arr,low,pivotIndex-1);\r\n         }\r\n         if(pivotIndex &lt; high) {\r\n             sort(arr,pivotIndex,high)\r\n         }   \r\n         return arr;\r\n     }\r\n \r\n     var arr = [4, 2, 6, 5, 3, 9];\r\n     var arrSorted = sort(arr);\r\n\r\n\r\nDuring first step , after swapping array is 4,2,3,5,6,9 .pivot index returned is 4. low becomes 0 in sort .How is high 3.\r\n\r\n\r\n",
            "link": "https://stackoverflow.com/questions/49791404/quicksort-hoare-partition-scheme",
            "title": "Quicksort Hoare Partition Scheme",
            "body": "<p>Implemented this Quick sort Algorithm in javascript</p>\n\n<pre><code>function  Quick_Sort(arr,l,h) {\n\n    var temp;\n    var low = l;\n    var high = h;\n    var pivot = arr[Math.floor((low+high)/2)];\n\n    while(low&lt;=high) {\n         while(arr[low] &lt; pivot) {\n            low++;\n         }\n        while(pivot &lt; arr[high]) {\n            high--;\n        }\n        if(low &lt;= high) {\n            temp = arr[low];  \n            arr[low] = arr[high];\n            arr[high] = temp;\n            low++;\n            high--;\n        }\n    }\n    return low;\n}\n\nfunction sort(arr,low,high) {\n\n    var low = low || 0;\n    var high = high || arr.length - 1;\n    var pivotIndex =  Quick_Sort(arr,low,high);\n\n    if(low &lt; pivotIndex-1) {   \n        sort(arr,low,pivotIndex-1);\n     }\n     if(pivotIndex &lt; high) {\n         sort(arr,pivotIndex,high)\n     }   \n     return arr;\n }\n\n var arr = [4, 2, 6, 5, 3, 9];\n var arrSorted = sort(arr);\n</code></pre>\n\n<p>During first step , after swapping array is 4,2,3,5,6,9 .pivot index returned is 4. low becomes 0 in sort .How is high 3.</p>\n"
        },
        {
            "tags": [
                "java",
                "java-8",
                "logback",
                "slf4j",
                "snmp4j"
            ],
            "owner": {
                "reputation": 1145,
                "user_id": 785523,
                "user_type": "registered",
                "accept_rate": 87,
                "profile_image": "https://www.gravatar.com/avatar/b51864374e248ee87686628bc8528677?s=128&d=identicon&r=PG",
                "display_name": "tuk",
                "link": "https://stackoverflow.com/users/785523/tuk"
            },
            "is_answered": false,
            "view_count": 11,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1523524118,
            "creation_date": 1523524118,
            "question_id": 49792459,
            "body_markdown": "We are using SLF4J and logback in our application code and SNMP4J has log4j as dependency.\r\n\r\nCan someone let me know how can I make SNMP4J work with logback?\r\n\r\nVersion\r\n\r\n - SNMP4J - 2.4.3\r\n - Logback - 1.2.3",
            "link": "https://stackoverflow.com/questions/49792459/using-snmp4j-with-logback",
            "title": "Using SNMP4J with logback",
            "body": "<p>We are using SLF4J and logback in our application code and SNMP4J has log4j as dependency.</p>\n\n<p>Can someone let me know how can I make SNMP4J work with logback?</p>\n\n<p>Version</p>\n\n<ul>\n<li>SNMP4J - 2.4.3</li>\n<li>Logback - 1.2.3</li>\n</ul>\n"
        },
        {
            "tags": [
                "php",
                "ajax",
                "amazon-web-services",
                "amazon-s3"
            ],
            "owner": {
                "reputation": 462,
                "user_id": 882857,
                "user_type": "registered",
                "accept_rate": 78,
                "profile_image": "https://www.gravatar.com/avatar/215a11cc9b4eefba77edc4ea57de7244?s=128&d=identicon&r=PG",
                "display_name": "Acidon",
                "link": "https://stackoverflow.com/users/882857/acidon"
            },
            "is_answered": false,
            "view_count": 60,
            "bounty_amount": 50,
            "bounty_closes_date": 1524070976,
            "answer_count": 2,
            "score": 2,
            "last_activity_date": 1523524106,
            "creation_date": 1523292109,
            "question_id": 49737749,
            "body_markdown": "I need to make a web form capable of uploading multiple files directly to my AWS S3 bucket from browser with PHP.\r\n\r\nI came across this nice solution for single file uploads (https://www.sanwebe.com/2015/09/direct-upload-to-amazon-aws-s3-using-php-html):\r\n\r\n    &lt;?php\r\n    $access_key         = &quot;iam-user-access-key&quot;; //Access Key\r\n    $secret_key         = &quot;iam-user-secret-key&quot;; //Secret Key\r\n    $my_bucket          = &quot;mybucket&quot;; //bucket name\r\n    $region             = &quot;us-east-1&quot;; //bucket region\r\n    $success_redirect   = &#39;http://&#39;. $_SERVER[&#39;SERVER_NAME&#39;] . $_SERVER[&#39;REQUEST_URI&#39;]; //URL to which the client is redirected upon success (currently self) \r\n    $allowd_file_size   = &quot;1048579&quot;; //1 MB allowed Size\r\n    \r\n    //dates\r\n    $short_date         = gmdate(&#39;Ymd&#39;); //short date\r\n    $iso_date           = gmdate(&quot;Ymd\\THis\\Z&quot;); //iso format date\r\n    $expiration_date    = gmdate(&#39;Y-m-d\\TG:i:s\\Z&#39;, strtotime(&#39;+1 hours&#39;)); //policy expiration 1 hour from now\r\n    \r\n    //POST Policy required in order to control what is allowed in the request\r\n    //For more info http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-HTTPPOSTConstructPolicy.html\r\n    $policy = utf8_encode(json_encode(array(\r\n                        &#39;expiration&#39; =&gt; $expiration_date,  \r\n                        &#39;conditions&#39; =&gt; array(\r\n                            array(&#39;acl&#39; =&gt; &#39;public-read&#39;),  \r\n                            array(&#39;bucket&#39; =&gt; $my_bucket), \r\n                            array(&#39;success_action_redirect&#39; =&gt; $success_redirect),\r\n                            array(&#39;starts-with&#39;, &#39;$key&#39;, &#39;&#39;),\r\n                            array(&#39;content-length-range&#39;, &#39;1&#39;, $allowd_file_size), \r\n                            array(&#39;x-amz-credential&#39; =&gt; $access_key.&#39;/&#39;.$short_date.&#39;/&#39;.$region.&#39;/s3/aws4_request&#39;),\r\n                            array(&#39;x-amz-algorithm&#39; =&gt; &#39;AWS4-HMAC-SHA256&#39;),\r\n                            array(&#39;X-amz-date&#39; =&gt; $iso_date)\r\n                            )))); \r\n    \r\n    //Signature calculation (AWS Signature Version 4)   \r\n    //For more info http://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html  \r\n    $kDate = hash_hmac(&#39;sha256&#39;, $short_date, &#39;AWS4&#39; . $secret_key, true);\r\n    $kRegion = hash_hmac(&#39;sha256&#39;, $region, $kDate, true);\r\n    $kService = hash_hmac(&#39;sha256&#39;, &quot;s3&quot;, $kRegion, true);\r\n    $kSigning = hash_hmac(&#39;sha256&#39;, &quot;aws4_request&quot;, $kService, true);\r\n    $signature = hash_hmac(&#39;sha256&#39;, base64_encode($policy), $kSigning);\r\n    ?&gt;\r\n    &lt;!DOCTYPE HTML&gt;\r\n    &lt;html&gt;\r\n    &lt;head&gt;\r\n    &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot;&gt;\r\n    &lt;title&gt;Aws S3 Direct File Uploader&lt;/title&gt;\r\n    &lt;/head&gt;\r\n    &lt;body&gt;\r\n    &lt;form action=&quot;http://&lt;?= $my_bucket ?&gt;.s3.amazonaws.com/&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt;\r\n    &lt;input type=&quot;hidden&quot; name=&quot;key&quot; value=&quot;${filename}&quot; /&gt;\r\n    &lt;input type=&quot;hidden&quot; name=&quot;acl&quot; value=&quot;public-read&quot; /&gt;\r\n    &lt;input type=&quot;hidden&quot; name=&quot;X-Amz-Credential&quot; value=&quot;&lt;?= $access_key; ?&gt;/&lt;?= $short_date; ?&gt;/&lt;?= $region; ?&gt;/s3/aws4_request&quot; /&gt;\r\n    &lt;input type=&quot;hidden&quot; name=&quot;X-Amz-Algorithm&quot; value=&quot;AWS4-HMAC-SHA256&quot; /&gt;\r\n    &lt;input type=&quot;hidden&quot; name=&quot;X-Amz-Date&quot; value=&quot;&lt;?=$iso_date ; ?&gt;&quot; /&gt;\r\n    &lt;input type=&quot;hidden&quot; name=&quot;Policy&quot; value=&quot;&lt;?=base64_encode($policy); ?&gt;&quot; /&gt;\r\n    &lt;input type=&quot;hidden&quot; name=&quot;X-Amz-Signature&quot; value=&quot;&lt;?=$signature ?&gt;&quot; /&gt;\r\n    &lt;input type=&quot;hidden&quot; name=&quot;success_action_redirect&quot; value=&quot;&lt;?= $success_redirect ?&gt;&quot; /&gt; \r\n    &lt;input type=&quot;file&quot; name=&quot;file&quot; /&gt;\r\n    &lt;input type=&quot;submit&quot; value=&quot;Upload File&quot; /&gt;\r\n    &lt;/form&gt;\r\n    &lt;?php\r\n    //After success redirection from AWS S3\r\n    if(isset($_GET[&quot;key&quot;]))\r\n    {\r\n        $filename = $_GET[&quot;key&quot;];\r\n        $ext = pathinfo($filename, PATHINFO_EXTENSION);\r\n        if(in_array($ext, array(&quot;jpg&quot;, &quot;png&quot;, &quot;gif&quot;, &quot;jpeg&quot;))){\r\n            echo &#39;&lt;hr /&gt;Image File Uploaded : &lt;br /&gt;&lt;img src=&quot;//&#39;.$my_bucket.&#39;.s3.amazonaws.com/&#39;.$_GET[&quot;key&quot;].&#39;&quot; style=&quot;width:100%;&quot; /&gt;&#39;;\r\n        }else{\r\n            echo &#39;&lt;hr /&gt;File Uploaded : &lt;br /&gt;&lt;a href=&quot;http://&#39;.$my_bucket.&#39;.s3.amazonaws.com/&#39;.$_GET[&quot;key&quot;].&#39;&quot;&gt;&#39;.$filename.&#39;&lt;/a&gt;&#39;;\r\n        }\r\n    }\r\n    ?&gt;\r\n    &lt;/body&gt;\r\n    &lt;/html&gt;\r\n\r\nIt works great for it purpose, but I need a solution that will be able to upload multiple uploads at once. \r\n\r\nOne of the comments on page specifies an approach:\r\n\r\n&gt; AWS only allows you to upload one file at a time if uploading directly to S3. You can do multi file uploads by setting the file input to multiple and looping through each of the files, making mulitple submissions via AJAX. To do this you need to set up CORS on the bucket you want to upload to, otherwise youll be denied on the grounds of it being a cross-site script. It can be accomplished, as Ive just got it working on my own project.\r\n\r\nI am trying to follow, but not sure how exactly he proposes to use an AJAX to make it work. Does the form will be on AJAx request page and I just feed file names to it? \r\n\r\nCan someone familiar with an issue can please explain it to me more thoroughly or direct me to the alternative solution(s)?\r\n\r\n ",
            "link": "https://stackoverflow.com/questions/49737749/upload-multiple-files-directly-to-aws-s3-bucket-from-browser-with-php",
            "title": "Upload multiple files directly to AWS S3 bucket from browser with PHP",
            "body": "<p>I need to make a web form capable of uploading multiple files directly to my AWS S3 bucket from browser with PHP.</p>\n\n<p>I came across this nice solution for single file uploads (<a href=\"https://www.sanwebe.com/2015/09/direct-upload-to-amazon-aws-s3-using-php-html\" rel=\"nofollow noreferrer\">https://www.sanwebe.com/2015/09/direct-upload-to-amazon-aws-s3-using-php-html</a>):</p>\n\n<pre><code>&lt;?php\n$access_key         = \"iam-user-access-key\"; //Access Key\n$secret_key         = \"iam-user-secret-key\"; //Secret Key\n$my_bucket          = \"mybucket\"; //bucket name\n$region             = \"us-east-1\"; //bucket region\n$success_redirect   = 'http://'. $_SERVER['SERVER_NAME'] . $_SERVER['REQUEST_URI']; //URL to which the client is redirected upon success (currently self) \n$allowd_file_size   = \"1048579\"; //1 MB allowed Size\n\n//dates\n$short_date         = gmdate('Ymd'); //short date\n$iso_date           = gmdate(\"Ymd\\THis\\Z\"); //iso format date\n$expiration_date    = gmdate('Y-m-d\\TG:i:s\\Z', strtotime('+1 hours')); //policy expiration 1 hour from now\n\n//POST Policy required in order to control what is allowed in the request\n//For more info http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-HTTPPOSTConstructPolicy.html\n$policy = utf8_encode(json_encode(array(\n                    'expiration' =&gt; $expiration_date,  \n                    'conditions' =&gt; array(\n                        array('acl' =&gt; 'public-read'),  \n                        array('bucket' =&gt; $my_bucket), \n                        array('success_action_redirect' =&gt; $success_redirect),\n                        array('starts-with', '$key', ''),\n                        array('content-length-range', '1', $allowd_file_size), \n                        array('x-amz-credential' =&gt; $access_key.'/'.$short_date.'/'.$region.'/s3/aws4_request'),\n                        array('x-amz-algorithm' =&gt; 'AWS4-HMAC-SHA256'),\n                        array('X-amz-date' =&gt; $iso_date)\n                        )))); \n\n//Signature calculation (AWS Signature Version 4)   \n//For more info http://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html  \n$kDate = hash_hmac('sha256', $short_date, 'AWS4' . $secret_key, true);\n$kRegion = hash_hmac('sha256', $region, $kDate, true);\n$kService = hash_hmac('sha256', \"s3\", $kRegion, true);\n$kSigning = hash_hmac('sha256', \"aws4_request\", $kService, true);\n$signature = hash_hmac('sha256', base64_encode($policy), $kSigning);\n?&gt;\n&lt;!DOCTYPE HTML&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"&gt;\n&lt;title&gt;Aws S3 Direct File Uploader&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;form action=\"http://&lt;?= $my_bucket ?&gt;.s3.amazonaws.com/\" method=\"post\" enctype=\"multipart/form-data\"&gt;\n&lt;input type=\"hidden\" name=\"key\" value=\"${filename}\" /&gt;\n&lt;input type=\"hidden\" name=\"acl\" value=\"public-read\" /&gt;\n&lt;input type=\"hidden\" name=\"X-Amz-Credential\" value=\"&lt;?= $access_key; ?&gt;/&lt;?= $short_date; ?&gt;/&lt;?= $region; ?&gt;/s3/aws4_request\" /&gt;\n&lt;input type=\"hidden\" name=\"X-Amz-Algorithm\" value=\"AWS4-HMAC-SHA256\" /&gt;\n&lt;input type=\"hidden\" name=\"X-Amz-Date\" value=\"&lt;?=$iso_date ; ?&gt;\" /&gt;\n&lt;input type=\"hidden\" name=\"Policy\" value=\"&lt;?=base64_encode($policy); ?&gt;\" /&gt;\n&lt;input type=\"hidden\" name=\"X-Amz-Signature\" value=\"&lt;?=$signature ?&gt;\" /&gt;\n&lt;input type=\"hidden\" name=\"success_action_redirect\" value=\"&lt;?= $success_redirect ?&gt;\" /&gt; \n&lt;input type=\"file\" name=\"file\" /&gt;\n&lt;input type=\"submit\" value=\"Upload File\" /&gt;\n&lt;/form&gt;\n&lt;?php\n//After success redirection from AWS S3\nif(isset($_GET[\"key\"]))\n{\n    $filename = $_GET[\"key\"];\n    $ext = pathinfo($filename, PATHINFO_EXTENSION);\n    if(in_array($ext, array(\"jpg\", \"png\", \"gif\", \"jpeg\"))){\n        echo '&lt;hr /&gt;Image File Uploaded : &lt;br /&gt;&lt;img src=\"//'.$my_bucket.'.s3.amazonaws.com/'.$_GET[\"key\"].'\" style=\"width:100%;\" /&gt;';\n    }else{\n        echo '&lt;hr /&gt;File Uploaded : &lt;br /&gt;&lt;a href=\"http://'.$my_bucket.'.s3.amazonaws.com/'.$_GET[\"key\"].'\"&gt;'.$filename.'&lt;/a&gt;';\n    }\n}\n?&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n\n<p>It works great for it purpose, but I need a solution that will be able to upload multiple uploads at once. </p>\n\n<p>One of the comments on page specifies an approach:</p>\n\n<blockquote>\n  <p>AWS only allows you to upload one file at a time if uploading directly to S3. You can do multi file uploads by setting the file input to multiple and looping through each of the files, making mulitple submissions via AJAX. To do this you need to set up CORS on the bucket you want to upload to, otherwise youll be denied on the grounds of it being a cross-site script. It can be accomplished, as Ive just got it working on my own project.</p>\n</blockquote>\n\n<p>I am trying to follow, but not sure how exactly he proposes to use an AJAX to make it work. Does the form will be on AJAx request page and I just feed file names to it? </p>\n\n<p>Can someone familiar with an issue can please explain it to me more thoroughly or direct me to the alternative solution(s)?</p>\n"
        },
        {
            "tags": [
                "javascript",
                "facebook",
                "reactjs",
                "performance",
                "dom"
            ],
            "owner": {
                "reputation": 428,
                "user_id": 5297860,
                "user_type": "registered",
                "accept_rate": 100,
                "profile_image": "https://www.gravatar.com/avatar/de25e2f82f50ed8961d02ab17a6a22d9?s=128&d=identicon&r=PG&f=1",
                "display_name": "luiswill",
                "link": "https://stackoverflow.com/users/5297860/luiswill"
            },
            "is_answered": true,
            "view_count": 52,
            "answer_count": 3,
            "score": -2,
            "last_activity_date": 1523524103,
            "creation_date": 1523522523,
            "last_edit_date": 1523523226,
            "question_id": 49791865,
            "body_markdown": "Please read this sequence on that [question (Update 5)][1] to understand the context : \r\n\r\n&gt; [React + Performance = ? article][2] by Paul Lewis from July 2015 shows an\r\n&gt; example where React is slower than vanilla JavaScript written by hand\r\n&gt; for an infinite list of Flickr pictures, which is especially\r\n&gt; significant on mobile. This example shows that everyone should always\r\n&gt; test performance for specific use case and specific target platforms\r\n&gt; and devices.\r\n\r\n\r\n\r\nI am asking myself, why would Facebook use React to manipulate their DOM ?\r\nIt seems that manipulating DOM is faster with vanilla JS, which is kind of logical because React is written with JS.\r\n\r\n**Isn&#39;t user experience much more important than developer ergonomics ?** \r\n\r\nEDIT : For example Amazon found out that [100ms of latency cost them 1% of sales][3]\r\n\r\n[![Vanilla vs React DOM manipulation performance][4]][4]\r\n\r\n\r\n  [1]: https://softwareengineering.stackexchange.com/questions/225400/pros-and-cons-of-facebooks-react-vs-web-components-polymer\r\n  [2]: https://aerotwist.com/blog/react-plus-performance-equals-what/\r\n  [3]: https://blog.gigaspaces.com/amazon-found-every-100ms-of-latency-cost-them-1-in-sales/\r\n  [4]: https://i.stack.imgur.com/9Mhlq.png",
            "link": "https://stackoverflow.com/questions/49791865/why-is-facebook-using-react-instead-of-more-performant-native-js",
            "title": "Why is Facebook using react instead of more performant native JS?",
            "body": "<p>Please read this sequence on that <a href=\"https://softwareengineering.stackexchange.com/questions/225400/pros-and-cons-of-facebooks-react-vs-web-components-polymer\">question (Update 5)</a> to understand the context : </p>\n\n<blockquote>\n  <p><a href=\"https://aerotwist.com/blog/react-plus-performance-equals-what/\" rel=\"nofollow noreferrer\">React + Performance = ? article</a> by Paul Lewis from July 2015 shows an\n  example where React is slower than vanilla JavaScript written by hand\n  for an infinite list of Flickr pictures, which is especially\n  significant on mobile. This example shows that everyone should always\n  test performance for specific use case and specific target platforms\n  and devices.</p>\n</blockquote>\n\n<p>I am asking myself, why would Facebook use React to manipulate their DOM ?\nIt seems that manipulating DOM is faster with vanilla JS, which is kind of logical because React is written with JS.</p>\n\n<p><strong>Isn't user experience much more important than developer ergonomics ?</strong> </p>\n\n<p>EDIT : For example Amazon found out that <a href=\"https://blog.gigaspaces.com/amazon-found-every-100ms-of-latency-cost-them-1-in-sales/\" rel=\"nofollow noreferrer\">100ms of latency cost them 1% of sales</a></p>\n\n<p><a href=\"https://i.stack.imgur.com/9Mhlq.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/9Mhlq.png\" alt=\"Vanilla vs React DOM manipulation performance\"></a></p>\n"
        },
        {
            "tags": [
                "ubuntu",
                "cuda"
            ],
            "owner": {
                "reputation": 166,
                "user_id": 1099376,
                "user_type": "registered",
                "profile_image": "https://i.stack.imgur.com/LrCX1.jpg?s=128&g=1",
                "display_name": "vambo",
                "link": "https://stackoverflow.com/users/1099376/vambo"
            },
            "is_answered": true,
            "view_count": 11659,
            "accepted_answer_id": 41410416,
            "answer_count": 2,
            "score": 4,
            "last_activity_date": 1523524100,
            "creation_date": 1483199996,
            "question_id": 41409842,
            "body_markdown": "I&#39;ve installed the latest nvidia drivers (375.26) manually, and installed CUDA using cuda_8.0.44_linux.run (skipping the driver install there, since the bundled drivers are older, 367 I think).\r\n\r\nRunning the deviceQuery in CUDA samples produces the following error however:\r\n\r\n    ~/cudasamples/NVIDIA_CUDA-8.0_Samples/1_Utilities/deviceQuery$ ./deviceQuery \r\n    ./deviceQuery Starting...\r\n    \r\n     CUDA Device Query (Runtime API) version (CUDART static linking)\r\n    \r\n    cudaGetDeviceCount returned 35\r\n    -&gt; CUDA driver version is insufficient for CUDA runtime version\r\n    Result = FAIL\r\n\r\nVersion info:\r\n\r\n$ nvcc --version\r\n\r\n    nvcc: NVIDIA (R) Cuda compiler driver\r\n    Copyright (c) 2005-2016 NVIDIA Corporation\r\n    Built on Sun_Sep__4_22:14:01_CDT_2016\r\n    Cuda compilation tools, release 8.0, V8.0.44\r\n    \r\n    $ nvidia-smi\r\n    Sat Dec 31 17:25:03 2016       \r\n    +-----------------------------------------------------------------------------+\r\n    | NVIDIA-SMI 375.26                 Driver Version: 375.26                    |\r\n    |-------------------------------+----------------------+----------------------+\r\n    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n    |===============================+======================+======================|\r\n    |   0  GeForce GTX 1080    Off  | 0000:01:00.0      On |                  N/A |\r\n    |  0%   39C    P8    11W / 230W |    464MiB /  8110MiB |      1%      Default |\r\n    +-------------------------------+----------------------+----------------------+\r\n                                                                                   \r\n    +-----------------------------------------------------------------------------+\r\n    | Processes:                                                       GPU Memory |\r\n    |  GPU       PID  Type  Process name                               Usage      |\r\n    |=============================================================================|\r\n    |    0       974    G   /usr/lib/xorg/Xorg                             193MiB |\r\n    |    0      1816    G   compiz                                         172MiB |\r\n    |    0      2178    G   ...ignDownloads/Enabled/MaterialDesignUserMa    96MiB |\r\n    +-----------------------------------------------------------------------------+\r\n    \r\n    $  cat /proc/driver/nvidia/version \r\n    NVRM version: NVIDIA UNIX x86_64 Kernel Module  375.26  Thu Dec  8 18:36:43 PST 2016\r\n    GCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4) \r\n\r\nThe anwer to similar problems has been updating the nvidia display drivers, though in my case this is already done. Does anyone have any ideas? Thanks.",
            "link": "https://stackoverflow.com/questions/41409842/ubuntu-16-04-cuda-8-cuda-driver-version-is-insufficient-for-cuda-runtime-vers",
            "title": "Ubuntu 16.04, CUDA 8 - CUDA driver version is insufficient for CUDA runtime version",
            "body": "<p>I've installed the latest nvidia drivers (375.26) manually, and installed CUDA using cuda_8.0.44_linux.run (skipping the driver install there, since the bundled drivers are older, 367 I think).</p>\n\n<p>Running the deviceQuery in CUDA samples produces the following error however:</p>\n\n<pre><code>~/cudasamples/NVIDIA_CUDA-8.0_Samples/1_Utilities/deviceQuery$ ./deviceQuery \n./deviceQuery Starting...\n\n CUDA Device Query (Runtime API) version (CUDART static linking)\n\ncudaGetDeviceCount returned 35\n-&gt; CUDA driver version is insufficient for CUDA runtime version\nResult = FAIL\n</code></pre>\n\n<p>Version info:</p>\n\n<p>$ nvcc --version</p>\n\n<pre><code>nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2016 NVIDIA Corporation\nBuilt on Sun_Sep__4_22:14:01_CDT_2016\nCuda compilation tools, release 8.0, V8.0.44\n\n$ nvidia-smi\nSat Dec 31 17:25:03 2016       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 1080    Off  | 0000:01:00.0      On |                  N/A |\n|  0%   39C    P8    11W / 230W |    464MiB /  8110MiB |      1%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0       974    G   /usr/lib/xorg/Xorg                             193MiB |\n|    0      1816    G   compiz                                         172MiB |\n|    0      2178    G   ...ignDownloads/Enabled/MaterialDesignUserMa    96MiB |\n+-----------------------------------------------------------------------------+\n\n$  cat /proc/driver/nvidia/version \nNVRM version: NVIDIA UNIX x86_64 Kernel Module  375.26  Thu Dec  8 18:36:43 PST 2016\nGCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4) \n</code></pre>\n\n<p>The anwer to similar problems has been updating the nvidia display drivers, though in my case this is already done. Does anyone have any ideas? Thanks.</p>\n"
        },
        {
            "tags": [
                "java",
                "spring-boot",
                "log4j",
                "cucumber-java"
            ],
            "owner": {
                "reputation": 59,
                "user_id": 2987384,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/db30279cbb47b6e9bba624db00e6ca25?s=128&d=identicon&r=PG&f=1",
                "display_name": "Eldhose",
                "link": "https://stackoverflow.com/users/2987384/eldhose"
            },
            "is_answered": false,
            "view_count": 13,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1523524095,
            "creation_date": 1523524095,
            "question_id": 49792453,
            "body_markdown": "How could I get the log statements which are written in the spring application while running the cucumber test?\r\n\r\nI am getting the logs written in the step definitions, but I am not getting the logs from actual code.\r\n\r\nThanks in advance.",
            "link": "https://stackoverflow.com/questions/49792453/logging-while-running-cucumber-tests",
            "title": "Logging while running cucumber tests",
            "body": "<p>How could I get the log statements which are written in the spring application while running the cucumber test?</p>\n\n<p>I am getting the logs written in the step definitions, but I am not getting the logs from actual code.</p>\n\n<p>Thanks in advance.</p>\n"
        },
        {
            "tags": [
                "reactjs",
                "react-native-web"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 8902394,
                "user_type": "registered",
                "profile_image": "https://graph.facebook.com/876356702527373/picture?type=large",
                "display_name": "Adrian Johnson",
                "link": "https://stackoverflow.com/users/8902394/adrian-johnson"
            },
            "is_answered": false,
            "view_count": 18,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1523524093,
            "creation_date": 1522441885,
            "question_id": 49581051,
            "body_markdown": "I&#39;m trying to set a function where when a button is clicked the program will send an email. I&#39;m using react native web. so far I have tried react-native-mail. this opens an email client but when I try to run build-web, the node module for react-native-mail can&#39;t be minified. I&#39;ve also tried react-native-communications but I get an error as well. Are there any methods I&#39;m missing?",
            "link": "https://stackoverflow.com/questions/49581051/react-native-web-send-an-email",
            "title": "React native web send an email",
            "body": "<p>I'm trying to set a function where when a button is clicked the program will send an email. I'm using react native web. so far I have tried react-native-mail. this opens an email client but when I try to run build-web, the node module for react-native-mail can't be minified. I've also tried react-native-communications but I get an error as well. Are there any methods I'm missing?</p>\n"
        },
        {
            "tags": [
                "arduino"
            ],
            "owner": {
                "reputation": 11,
                "user_id": 9582200,
                "user_type": "registered",
                "profile_image": "https://lh4.googleusercontent.com/-bZDNxyHsxHM/AAAAAAAAAAI/AAAAAAAAABA/aXWg-oHJRno/photo.jpg?sz=128",
                "display_name": "Ilya Savitsky",
                "link": "https://stackoverflow.com/users/9582200/ilya-savitsky"
            },
            "is_answered": false,
            "view_count": 19,
            "answer_count": 1,
            "score": 1,
            "last_activity_date": 1523524090,
            "creation_date": 1523281847,
            "last_edit_date": 1523329313,
            "question_id": 49734618,
            "body_markdown": "I tried to upload my code to an Arduino Leonardo through  theArduino IDE, but:\r\n\r\n    avrdude: butterfly_recv(): programmer is not responding\r\n    avrdude: butterfly_recv(): programmer is not responding\r\n    avrdude: butterfly_recv(): programmer is not responding\r\n    avrdude: butterfly_recv(): programmer is not responding\r\n    avrdude: butterfly_recv(): programmer is not responding\r\n    Found programmer: Id = &quot;&quot;; type = \r\n    Software Version = h.; Hardware Version = u.M\r\n    avrdude: butterfly_recv(): programmer is not responding\r\n    avrdude: butterfly_recv(): programmer is not responding\r\n    avrdude: error: buffered memory access not supported. Maybe it isn&#39;t\r\n    a butterfly/AVR109 but a AVR910 device?\r\n    avrdude: initialization failed, rc=-1\r\n             Double check connections and try again, or use -F to override\r\n             this check.\r\n    \r\n    avrdude: butterfly_recv(): programmer is not responding\r\n    avrdude: error: programmer did not respond to command: leave prog mode\r\n    avrdude: butterfly_recv(): programmer is not responding\r\n    avrdude: error: programmer did not respond to command: exit bootloader\r\n    avrdude: error: programmer did not respond to command: exit bootloader\r\n\r\nWhat is wrong? How do I fix this?",
            "link": "https://stackoverflow.com/questions/49734618/arduino-leonardo-error-exiting-bootloader",
            "title": "Arduino Leonardo: error exiting bootloader",
            "body": "<p>I tried to upload my code to an Arduino Leonardo through  theArduino IDE, but:</p>\n\n<pre><code>avrdude: butterfly_recv(): programmer is not responding\navrdude: butterfly_recv(): programmer is not responding\navrdude: butterfly_recv(): programmer is not responding\navrdude: butterfly_recv(): programmer is not responding\navrdude: butterfly_recv(): programmer is not responding\nFound programmer: Id = \"\"; type = \nSoftware Version = h.; Hardware Version = u.M\navrdude: butterfly_recv(): programmer is not responding\navrdude: butterfly_recv(): programmer is not responding\navrdude: error: buffered memory access not supported. Maybe it isn't\na butterfly/AVR109 but a AVR910 device?\navrdude: initialization failed, rc=-1\n         Double check connections and try again, or use -F to override\n         this check.\n\navrdude: butterfly_recv(): programmer is not responding\navrdude: error: programmer did not respond to command: leave prog mode\navrdude: butterfly_recv(): programmer is not responding\navrdude: error: programmer did not respond to command: exit bootloader\navrdude: error: programmer did not respond to command: exit bootloader\n</code></pre>\n\n<p>What is wrong? How do I fix this?</p>\n"
        },
        {
            "tags": [
                "project-reactor",
                "reactor"
            ],
            "owner": {
                "reputation": 660,
                "user_id": 6180987,
                "user_type": "registered",
                "accept_rate": 83,
                "profile_image": "https://i.stack.imgur.com/eAIPQ.jpg?s=128&g=1",
                "display_name": "Sergey Luchko",
                "link": "https://stackoverflow.com/users/6180987/sergey-luchko"
            },
            "is_answered": true,
            "view_count": 14,
            "accepted_answer_id": 49791575,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1523524089,
            "creation_date": 1523517468,
            "last_edit_date": 1523524089,
            "question_id": 49790289,
            "body_markdown": "I have list `List&lt;Mono&lt;String&gt;&gt;`. Each Mono represents API call where I wait on I/O for result. The problem is that some times some calls return nothing (empty String), and I need repeat them again on that case.\r\n\r\nNow it looks like this:\r\n\r\n    val firstAskForItemsRetrieved = firstAskForItems.map {\r\n        it[&quot;statistic&quot;] = (it[&quot;statistic&quot;] as Mono&lt;Map&lt;Any, Any&gt;&gt;).block()\r\n        it\r\n    }\r\n\r\nI&#39;m waiting for all Monos to finish, then in case of empty body I repeat request\r\n\r\n    val secondAskForItem = firstAskForItemsRetrieved\r\n            .map {\r\n                if ((it[&quot;statistic&quot;] as Map&lt;Any, Any&gt;).isEmpty()) {\r\n                    // repeat request \r\n                    it[&quot;statistic&quot;] = getUserItem(userName) // return Mono\r\n                } else\r\n                    it[&quot;statistic&quot;] = Mono.just(it[&quot;statistic&quot;])\r\n                it\r\n            }\r\n\r\nAnd then block on each item again\r\n\r\n    val secondAskForItemsRetrieved = secondAskForItems.map {\r\n        it[&quot;statistic&quot;] = (it[&quot;statistic&quot;] as Mono&lt;Map&lt;Any, Any&gt;&gt;).block()\r\n        it\r\n    }\r\n \r\nI see that looks ugly\r\n\r\n* Are any other ways to retry call in Mono if it fails, without doing it manually?\r\n\r\n* Is it block on each item a right way to get them all?\r\n\r\n* How to make the code better?\r\n\r\nThank you.",
            "link": "https://stackoverflow.com/questions/49790289/reactor-list-of-monos-retry-on-fail",
            "title": "Reactor. List of Monos, retry on fail",
            "body": "<p>I have list <code>List&lt;Mono&lt;String&gt;&gt;</code>. Each Mono represents API call where I wait on I/O for result. The problem is that some times some calls return nothing (empty String), and I need repeat them again on that case.</p>\n\n<p>Now it looks like this:</p>\n\n<pre><code>val firstAskForItemsRetrieved = firstAskForItems.map {\n    it[\"statistic\"] = (it[\"statistic\"] as Mono&lt;Map&lt;Any, Any&gt;&gt;).block()\n    it\n}\n</code></pre>\n\n<p>I'm waiting for all Monos to finish, then in case of empty body I repeat request</p>\n\n<pre><code>val secondAskForItem = firstAskForItemsRetrieved\n        .map {\n            if ((it[\"statistic\"] as Map&lt;Any, Any&gt;).isEmpty()) {\n                // repeat request \n                it[\"statistic\"] = getUserItem(userName) // return Mono\n            } else\n                it[\"statistic\"] = Mono.just(it[\"statistic\"])\n            it\n        }\n</code></pre>\n\n<p>And then block on each item again</p>\n\n<pre><code>val secondAskForItemsRetrieved = secondAskForItems.map {\n    it[\"statistic\"] = (it[\"statistic\"] as Mono&lt;Map&lt;Any, Any&gt;&gt;).block()\n    it\n}\n</code></pre>\n\n<p>I see that looks ugly</p>\n\n<ul>\n<li><p>Are any other ways to retry call in Mono if it fails, without doing it manually?</p></li>\n<li><p>Is it block on each item a right way to get them all?</p></li>\n<li><p>How to make the code better?</p></li>\n</ul>\n\n<p>Thank you.</p>\n"
        }
    ],
    "has_more": true,
    "quota_max": 300,
    "quota_remaining": 24
}